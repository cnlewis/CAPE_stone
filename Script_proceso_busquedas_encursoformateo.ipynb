{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargamos librerías y definimos variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cargamos librerías\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from textblob import TextBlob\n",
    "import langdetect as lang\n",
    "import pickle\n",
    "import sys\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import enchant as ench\n",
    "from stop_words import get_stop_words\n",
    "import string\n",
    "import inflection\n",
    "import inflect\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "\n",
    "#Definimos variables de entorno\n",
    "\n",
    "base_dir = r'D:\\Capstone-Wordreader\\Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga, limpieza y selección de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos los procesos necesarios para normalizar el texto y detectar el idioma en el que ha sido escrita cada búsqueda:\n",
    "\n",
    "* Recuento de búsquedas por usuario\n",
    "* Carga y limpieza de datos: signos, números al principio de frase y celdas vacías\n",
    "* Detección de idioma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  ##import pandas as pd\n",
    "  ##import re\n",
    "  ##from textblob import TextBlob\n",
    "  ##import langdetect as lang\n",
    "\n",
    "\n",
    "#cargamos fichero y seleccionamos campos\n",
    "\n",
    "\n",
    "##df = pd.read_csv('C:/Users/Paqui/Programas Python/Capstone project/queries003.csv')\n",
    "df = pd.read_csv(os.path.join(base_dir,queries003.csv))\n",
    "df1 = df[['customer','country','query']]\n",
    "\n",
    "\n",
    "#contamos el número de búsquedas por usuarios\n",
    "\n",
    "df_busc_user= df1.groupby('customer').count()\n",
    "media_busquedas= df_busc_user.mean()\n",
    "\n",
    "\n",
    "#contamos el número de diferentes búsquedas por usuarios\n",
    "\n",
    "df_difbusc_user= df1.groupby('customer')['query'].nunique()\n",
    "media_difbusquedas= df_difbusc_user.mean()\n",
    "\n",
    "\n",
    "#Creamos un dataframe agrupado por customer y query\n",
    "\n",
    "df2 = pd.DataFrame({'count' : df1.groupby( [ \"customer\", \"country\", \"query\"] ).size()}).reset_index()\n",
    "print df2.head()\n",
    "\n",
    "\n",
    "#Contamos con cuantas querys nos hemos quedado\n",
    "\n",
    "print \"Querys no unificadas\", df1['query'].count()\n",
    "print \"Querys unificadas\",    df2['query'].count()\n",
    "\n",
    "\n",
    "#Eliminamos los special caracters, menos los espacios y los apóstrofes\n",
    "#Con la función strip nos quitamos los espacios al inicio y al final de la línea\n",
    "\n",
    "df2['query_2'] = df2['query'].map(lambda x: re.sub(r\"[^\\w' ]\", '', x))\n",
    "df2['query_2'] = df2['query_2'].map(lambda x: re.sub(r\"_\", '', x))\n",
    "df2['query_2'] = df2['query_2'].str.strip()\n",
    "df2.head()\n",
    "print df2.head()\n",
    "\n",
    "\n",
    "#Eliminamos las querys que contienen solo números con espocios o solo espacios y contamos cuantas tenemos\n",
    "\n",
    "df2['query_2'] = df2['query_2'].map(lambda x: re.sub(r\"^[0-9 ]+$\", '', x))\n",
    "print df2.head()\n",
    "print \"Querys no numéricas\",    df2['query_2'].count()\n",
    "\n",
    "\n",
    "#Eliminamos las filas con texto vacío\n",
    "df3 = df2[df2['query_2'] != \"\"]\n",
    "print \"Querys no vacías\",    df3['query_2'].count()\n",
    "print df3.head()\n",
    "\n",
    "#Si la query tiene menos de 3 carácteres entonces usamos el langdetect porque el otro da error\n",
    "\n",
    "def detect_lang(query):\n",
    "    if len(query) > 3:\n",
    "        return TextBlob(query).detect_language()\n",
    "    else:\n",
    "        return lang.detect(query)\n",
    "    \n",
    "\n",
    "df3['language'] = df3['query_2'].apply(detect_lang)\n",
    "df3.head()\n",
    "\n",
    "##df3.to_csv('C:/Users/Paqui/Programas Python/Capstone project/queries003_corrected.csv')\n",
    "df3.to_csv(os.path.join(base_dir,queries003_corrected.csv))\n",
    "\n",
    "print 'FINISHED'\n",
    "\"\"\"\n",
    "for i in xrange(312112,315655):\n",
    "    text = df3.iloc[i,3]\n",
    "    print text\n",
    "    language = lang.detect(text)\n",
    "    print i, \": \", language\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización de búsquedas en inglés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos el inglés como idioma de trabajo y buscamos cada uno de las palabras normalizadas en tres librerías: _Enchant_, _nltk_ y diccionario Python\n",
    "\n",
    "* Leemos el fichero corregido y con el idioma asignado y nos quedamos con los campos que nos interesan\n",
    "* Seleccionamos las queries en inglés y vemos cuantas hay\n",
    "* Creamos un variable string juntando todas las queries. Ponemos en minúsculas todas las palabras\n",
    "* Seleccionamos el inglés como idioma para la libreria _Enchant_ y _nltk_\n",
    "* Buscamos cada palabra en el diccionario de Python y controlamos si es encontrada en _Enchant_, _nltk_ y diccionario Python\n",
    "* Generamos un fichero de registro de palabras erróneas corregidas y pendientes de corregir\n",
    "* Contamos el número de apariciones de cada término corregido para construir el diccionario con el que corregiremos las palabras escritas erróneamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named P02B_Spelling_corrector6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c5ff70f98dca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:/Users/Paqui/Programas Python/Capstone project'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mP02B_Spelling_corrector6\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspelling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named P02B_Spelling_corrector6"
     ]
    }
   ],
   "source": [
    "  ##import pandas as pd\n",
    "  ##import pickle\n",
    "  ##import sys\n",
    "\n",
    "  ##sys.path.append('C:/Users/Paqui/Programas Python/Capstone project')\n",
    "sys.path.append(base_dir)\n",
    "\n",
    "import P02B_Spelling_corrector6 as spelling\n",
    "\n",
    "  ##df = pd.read_csv('C:/Users/Paqui/Programas Python/Capstone project/queries001.csv')\n",
    "df = pd.read_csv(os.path.join(base_dir,queries001_corrected.csv))\n",
    "\n",
    "df1 = df[['customer','query_2', 'language', 'country']]\n",
    "\n",
    "df1.head()\n",
    "\n",
    "print 'Queries con todos los idiomas:',df1['query_2'].count()\n",
    "\n",
    "df_english = df1[df1.language == 'en']\n",
    "print df_english.head()\n",
    "\n",
    "print 'Queries con idioma inglés :',df_english['query_2'].count()\n",
    "\n",
    "  ##inputfile = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/errors001_counter', 'rb')\n",
    "inputfile = open(os.path.join(base_dir,errors001_counter), 'rb')\n",
    "    \n",
    "errors = pickle.load(inputfile)\n",
    "\n",
    "errors_str = \" \".join(str(x) for x in errors)\n",
    "\n",
    "print errors_str\n",
    "\n",
    "def correct_spelling(query):\n",
    "    words = query.split()\n",
    "    query_2 = \"\"\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        corrected_word = \"\"\n",
    "        single_word = \" \" + word.lower() + \" \" \n",
    "        \"We want to match the whole single  word in the words errors string\"\n",
    "        if single_word in errors_str:\n",
    "            corrected_word = spelling.correction(word.lower())\n",
    "            query_2 = query_2 + \" \" + corrected_word\n",
    "        else: \n",
    "            query_2 = query_2 + \" \" + word\n",
    "    \n",
    "    return query_2\n",
    "\n",
    "df_english['query_spel_corr'] = df_english['query_2'].apply(lambda query: correct_spelling(query))\n",
    "\n",
    "\n",
    "# Creamos un dataframe agrupado por customer y query para eliminar duplicados una vez corregido\n",
    "\n",
    "df_english_corr = pd.DataFrame({'count' : df_english.groupby( [ \"customer\", \"country\", \"query_spel_corr\", \"language\"] ).size()}).reset_index()\n",
    "\n",
    "\n",
    "# Contamos con cuantas querys nos hemos quedado\n",
    "\n",
    "print \"Querys corregidas no unificadas\", df_english['query_spel_corr'].count()\n",
    "print \"Querys coregidas  unificadas\",    df_english_corr['query_spel_corr'].count()\n",
    "\n",
    "\n",
    "  ##df_english.to_csv('C:/Users/Paqui/Programas Python/Capstone project/queries001_spell_corrected_comparison.csv')\n",
    "  ##df_english_corr.to_csv('C:/Users/Paqui/Programas Python/Capstone project/queries001_spell_corrected.csv')\n",
    "df_english.to_csv(os.path.join(base_dir,queries001_spell_corrected_comparison.csv))\n",
    "df_english_corr.to_csv(os.path.join(base_dir,queries001_spell_corrected.csv))\n",
    "\n",
    "\n",
    "print 'FINISHED'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las funciones de corrección de palabras:\n",
    "* Creamos un contador de palabras agregando los contadores generados en cada uno de los archivos procesados\n",
    "* Definimos las funciones de búsqueda y corrección de palabra/s errónea/s en base a la frecuencia de recuento de términos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named enchant",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-300535e8382e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0menchant\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mench\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named enchant"
     ]
    }
   ],
   "source": [
    "  ##from collections import Counter\n",
    "  ##import pickle\n",
    "  ##import numpy as np\n",
    "  ##import enchant as ench\n",
    "\n",
    "diction = ench.Dict(\"en_UK\")\n",
    "\n",
    "#Abrimos los  contadores de palabras procesados anteriormente para cada uno de los ficheros\n",
    "\n",
    "  ##inputfile_001 = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/counter', 'rb')\n",
    "inputfile_001 = open(os.path.join(base_dir,counter), 'rb')\n",
    "    \n",
    "WORDS_001 = Counter (pickle.load(inputfile_001))\n",
    "        \n",
    "  ##inputfile_002 = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/counter_002', 'rb')\n",
    "inputfile_002 = open(os.path.join(base_dir,counter_002), 'rb')\n",
    "\n",
    "WORDS_002 = Counter (pickle.load(inputfile_002))\n",
    "\n",
    "  ##inputfile_003 = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/counter_003', 'rb')\n",
    "inputfile_003 = open(os.path.join(base_dir,counter_003), 'rb')\n",
    "    \n",
    "WORDS_003 = Counter (pickle.load(inputfile_003))\n",
    "\n",
    "\n",
    "#Creamos un contador sumando los contadores individuales de cada archivo\n",
    "\n",
    "WORDS = WORDS_001 + WORDS_002 + WORDS_003\n",
    "\n",
    "print  'Number of different words: ' ,len(WORDS.keys())\n",
    "print  'Sum of values of words:', sum(WORDS.values())\n",
    "\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`. No dividimos por el número de palabras de esta forma\"\n",
    "    \"nos evitamos probabilidades cercanas a 0\"\n",
    "    return WORDS[word]\n",
    " \n",
    "def correction(word): \n",
    "    \"We transform the word to lower case\"\n",
    "    word = word.lower()\n",
    "    \"Most probable spelling correction for word.\"\n",
    "    corrected_word1 = max(candidates(word), key=P)\n",
    "    \"Probability of the word.\"\n",
    "    p1 = P(corrected_word1)\n",
    "    \n",
    "    \"Most probable spelling correction for the word splitting subset.\"\n",
    "    corrected_word2 = max_split(word)\n",
    "    \n",
    "    \"If word was not corrected using the basic corrector we use enchant suggestions and we\"\n",
    "    \"select the one with more probability to appear in our text\"\n",
    "    if corrected_word1 == word:\n",
    "        suggestions = diction.suggest(word)\n",
    "        p_aux = 0\n",
    "        p_sug = 0\n",
    "        word_sug = \"\"\n",
    "        for item  in suggestions:\n",
    "            p_aux = P(item)\n",
    "            if p_aux > p_sug:\n",
    "                p_sug = p_aux\n",
    "                word_sug = item\n",
    "        \"We select depending on probability the corrected word or the two words form the splitting subset\"\n",
    "        if corrected_word2[1] > p_sug:\n",
    "            return corrected_word2[0][0] + \" \" + corrected_word2[0][1]\n",
    "        else:\n",
    "            if p_sug > 0:\n",
    "                return word_sug\n",
    "            else:\n",
    "                return word\n",
    "    \"If word corrected with the basic corrector then we return the word\"   \n",
    "    if corrected_word1 != word:\n",
    "        \"We select depending on probability the corrected word or the two words form the splitting subset\"\n",
    "        if corrected_word2[1] > p1:\n",
    "            return corrected_word2[0][0] + \" \" + corrected_word2[0][1]\n",
    "        else:\n",
    "            return corrected_word1\n",
    "\n",
    "        \n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "   \n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    \n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"All word splits.\"\n",
    "    \n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    \n",
    "  \n",
    "    return splits\n",
    "     \n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "\n",
    "def max_split(word):\n",
    "    \"All word splits.\"\n",
    "    words_splits = splits(word)\n",
    "    \n",
    "    p = np.zeros(len(words_splits))\n",
    "    \n",
    "    \"Calculating the sum of probabilities of all word splits, the two words must have\" \n",
    "    \"probability bigger than 0 (appear at least one time in the words dictionry\"\n",
    "    \"The words must have a length bigger than one to avoid single letters\"\n",
    "    i = 0\n",
    "    for word in words_splits:\n",
    "        p_0 = P(word[0])\n",
    "        p_1 = P(word[1])\n",
    "        if (p_0 > 0 and len(word[0] )> 1)  and (p_1 >0 and len(word[1])>1):\n",
    "            p[i] = p_0 + p_1\n",
    "        else: \n",
    "            p[i] = 0\n",
    "       \n",
    "        i = i + 1\n",
    "  \n",
    "    j = p.argmax()\n",
    "\n",
    "     \n",
    "    return (words_splits[j], p[j])\n",
    "\n",
    "print correction('CHIDREN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corregimos palabras erróneas:\n",
    "* Creamos un dataframe agrupado por customer y query para eliminar duplicados una vez corregido\n",
    "* Identificamos las palabras erróneas \"corregidas\" vs. \"no corregidas\" \n",
    "* Hacemos el recuento de búsquedas corregidas y no corregidas\n",
    "* Comparamos las búsquedas corregidas en base a la frecuencia de palabras vs. las búsquedas corregidas con el diccionario de términos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named enchant",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1aba5899e5fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0menchant\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mench\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named enchant"
     ]
    }
   ],
   "source": [
    "  ##import pandas as pd\n",
    "  ##import pickle\n",
    "  ##import sys\n",
    "\n",
    "  ##sys.path.append('C:/Users/Paqui/Programas Python/Capstone project')\n",
    "\n",
    "sys.path.append(base_dir)\n",
    "import P02B_Spelling_corrector6 as spelling\n",
    "\n",
    "  ##df = pd.read_csv('C:/Users/Paqui/Programas Python/Capstone project/queries001_corrected.csv')\n",
    "df = pd.read_csv(os.path.join(base_dir,queries001_corrected.csv))\n",
    "\n",
    "df1 = df[['customer','query_2', 'language', 'country']]\n",
    "\n",
    "df1.head()\n",
    "\n",
    "print 'Queries con todos los idiomas:',df1['query_2'].count()\n",
    "\n",
    "df_english = df1[df1.language == 'en']\n",
    "print df_english.head()\n",
    "\n",
    "print 'Queries con idioma inglés :',df_english['query_2'].count()\n",
    "\n",
    "  ##inputfile = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/errors001_counter', 'rb')\n",
    "inputfile = open(os.path.join(base_dir,errors001_counter), 'rb')\n",
    "\n",
    "errors = pickle.load(inputfile)\n",
    "\n",
    "errors_str = \" \".join(str(x) for x in errors)\n",
    "\n",
    "print errors_str\n",
    "\n",
    "def correct_spelling(query):\n",
    "    words = query.split()\n",
    "    query_2 = \"\"\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        corrected_word = \"\"\n",
    "        single_word = \" \" + word.lower() + \" \" \n",
    "        \"We want to match the whole single  word in the words errors string\"\n",
    "        if single_word in errors_str:\n",
    "            corrected_word = spelling.correction(word.lower())\n",
    "            query_2 = query_2 + \" \" + corrected_word\n",
    "        else: \n",
    "            query_2 = query_2 + \" \" + word\n",
    "    \n",
    "    return query_2\n",
    "\n",
    "\n",
    "df_english['query_spel_corr'] = df_english['query_2'].apply(lambda query: correct_spelling(query))\n",
    "\n",
    "\n",
    "#Creamos un dataframe agrupado por customer y query para eliminar duplicados una vez corregido\n",
    "\n",
    "df_english_corr = pd.DataFrame({'count' : df_english.groupby( [ \"customer\", \"country\", \"query_spel_corr\", \"language\"] ).size()}).reset_index()\n",
    "\n",
    "\n",
    "#Contamos con cuantas querys nos hemos quedado\n",
    "\n",
    "print \"Querys corregidas no unificadas\", df_english['query_spel_corr'].count()\n",
    "print \"Querys coregidas  unificadas\",    df_english_corr['query_spel_corr'].count()\n",
    "\n",
    "\n",
    "  ##df_english.to_csv('C:/Users/Paqui/Programas Python/Capstone project/queries001_spell_corrected_comparison.csv')\n",
    "  ##df_english_corr.to_csv('C:/Users/Paqui/Programas Python/Capstone project/queries001_spell_corrected.csv')\n",
    "\n",
    "df_english.to_csv(os.path.join(base_dir,queries001_spell_corrected_comparison.csv)\n",
    "df_english_corr.to_csv(os.path.join(base_dir,queries001_spell_corrected.csv)\n",
    "\n",
    "print 'FINISHED'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentación de temas sobre una muestra de 20.000 búsquedas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos una muestra de 20.000 búsquedas del fichero de búsquedas normalizadas corregidas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4d1b61146d55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Paqui/Programas Python/Capstone project/Counter copy/queries001_spell_corrected.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Paqui/Programas Python/Capstone project/Counter copy/queries002_spell_corrected.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Paqui/Programas Python/Capstone project/Counter copy/queries003_spell_corrected.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#cargamos ficheros y seleccionamos campos\n",
    "\n",
    "  ##df1 = pd.read_csv('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/queries001_spell_corrected.csv')\n",
    "  ##df2 = pd.read_csv('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/queries002_spell_corrected.csv')\n",
    "  ##df3 = pd.read_csv('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/queries003_spell_corrected.csv')\n",
    "df1 = pd.read_csv(os.path.join(base_dir,queries001_spell_corrected.csv)\n",
    "df2 = pd.read_csv(os.path.join(base_dir,queries002_spell_corrected.csv)\n",
    "df3 = pd.read_csv(os.path.join(base_dir,queries003_spell_corrected.csv)\n",
    "\n",
    "df3 = df3.iloc[0:3000]\n",
    "\n",
    "frames = [df1, df2, df3]\n",
    "\n",
    "df= pd.concat(frames)\n",
    "\n",
    "df_sample = df.sample(20000)\n",
    "\n",
    "  ##df_sample.to_csv('C:/Users/Paqui/Programas Python/Capstone project/queries123sc_sample.csv')\n",
    "df_sample.to_csv(os.path.join(base_dir,queries123sc_sample.csv)\n",
    "                  \n",
    "print 'FINISHED'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos un análisis descriptivo de los términos más utilizados en la muestra de búsquedas:\n",
    "* Comparando la distribución de búsquedas por país del fichero total en relación al fichero de muestra\n",
    "* Comparando la distribución de términos de la muestra respecto a la distribución de término del fichero completo (de búquedas)\n",
    "* Identificando los \"n\" primeros bigramas en funció de su frecuencia de aparición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comparamos la distribución de búsquedas por país del total respecto a la muestra\n",
    "\"\"\"\n",
    "\n",
    "#Contamos el número de búsquedas por pais y ordenamos de mayor a menor\n",
    "\n",
    "df_busc_country= df.groupby('country').count()\n",
    "df_busc_country\n",
    "\n",
    "df_busc_country_sort=df_busc_country.sort(['customer'],ascending=[False])\n",
    "df_busc_country_sort\n",
    "\n",
    "df_busc_country_sort.plot(kind='bar')\n",
    "\n",
    "df_busc_country_Top30=df_busc_country.sort(['customer'],ascending=[False]).head(30)\n",
    "df_busc_country_Top30.plot(kind='bar')\n",
    "\n",
    "\n",
    "#Contamos el número de búsquedas por pais de la muestra y ordenamos de mayor a menor\n",
    "\n",
    "df_busc_country_sample= df_sample.groupby('country').count()\n",
    "df_busc_country_sample\n",
    "\n",
    "df_busc_country_sample_sort=df_busc_country_sample.sort(['customer'],ascending=[False])\n",
    "df_busc_country_sample_sort\n",
    "\n",
    "df_busc_country_sample_sort.plot(kind='bar')\n",
    "\n",
    "df_busc_country_sample_Top30=df_busc_country_sample.sort(['customer'],ascending=[False]).head(30)\n",
    "df_busc_country_sample_Top30.plot(kind='bar')\n",
    "\n",
    "\n",
    "#comparamos Top30 ficheros con Top30 muestra\n",
    "\n",
    "df_busc_country_Top30.plot(kind='bar')\n",
    "df_busc_country_sample_Top30.plot(kind='bar')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Comparamos la distribución de términos de la muestra respecto a la distribución de término del fichero completo (de búquedas)\n",
    "\"\"\"\n",
    "\n",
    "#Pendiente de incorporar código desarrollado por Patricia\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Realizamos un análisis mediante bigramas del TOP \"n\" búsquedas de acuerdo con su frecuencia de aparición\n",
    "\"\"\"\n",
    "\n",
    "  ##import requests\n",
    "  ##import pandas as pd\n",
    "  ##from stop_words import get_stop_words\n",
    "  ##import string\n",
    "  ##from textblob import TextBlob\n",
    "  ##import inflection\n",
    "  ##import inflect\n",
    "  ##import re\n",
    "  ##from collections import Counter\n",
    "  ##import time\n",
    "  ##import matplotlib.pyplot as plt\n",
    "  ##import numpy as np\n",
    "  ##from nltk.stem.porter import PorterStemmer\n",
    "  ##import nltk\n",
    "\n",
    "\n",
    "#Importamos los CSVs y los unimos\n",
    "  ##df_words = pd.read_csv(r'C:\\Users\\apascual\\Documents\\Python_Scripts\\Worldreader\\queries123sc_sample.csv')\n",
    "df_words = pd.read_csv(os.path.join(base_dir,queries123sc_sample.csv)\n",
    "\n",
    "df_words003 = df_words[['customer','query_spel_corr', 'language', 'country', 'Index']]\n",
    "\n",
    "#Unimos los Csvs\n",
    "frames = [df_words003]\n",
    "df_words003 = pd.concat(frames)\n",
    "\n",
    "#Seleccionamos las columans que nos interesan\n",
    "df_words1 = df_words003[['customer','query_spel_corr', 'language','country', 'Index']]\n",
    "doc_complete = df_words1['query_spel_corr'].tolist()\n",
    "\n",
    "#Tokenizamos y sacamos las stopwords\n",
    "\n",
    "tokens = nltk.word_tokenize(str(doc_complete))\n",
    "stopwords = nltk.corpus.stopwords.words('english') \n",
    "tokens_nostopwords = [w for w in tokens if not w in stopwords]\n",
    "print tokens_nostopwords\n",
    "\n",
    "#Quitamos las puntuaciones\n",
    "string.punctuation\n",
    "tokens_nopuncnostopwords = [w.lower() for w in tokens_nostopwords if w.isalnum()]\n",
    "print tokens_nopuncnostopwords\n",
    "\n",
    "#encontramos los bigramas y contamos su frecuencia\n",
    "from itertools import islice, izip\n",
    "print Counter(izip(tokens_nopuncnostopwords, islice(tokens_nopuncnostopwords, 1, None)))\n",
    "bigrams_count= Counter(izip(tokens_nopuncnostopwords, islice(tokens_nopuncnostopwords, 1, None))) \n",
    "\n",
    "#ordenamos y pintamos los bigramas\n",
    "bigrams_plot = dict(bigrams_count.most_common(10))\n",
    "print(bigrams_plot)\n",
    "labels, values = zip(*bigrams_plot.items())\n",
    "indSort = np.argsort(values)[::-1] #ordenamos valores en orden descendente\n",
    "\n",
    "# redistribuimos los datos\n",
    "labels = np.array(labels)[indSort]\n",
    "values = np.array(values)[indSort]\n",
    "indexes = np.arange(len(labels))\n",
    "bar_width = 0.35\n",
    "plt.bar(indexes, values)\n",
    "\n",
    "# añadimos etiquetas\n",
    "plt.xticks(indexes + bar_width, labels)\n",
    "\n",
    "# mostramos gráficos\n",
    "plt.show()\n",
    "                       \n",
    "print 'FINISHED'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamamos a la API de Google Books para recuperar información del libro: título, autor y descripción\n",
    "* Seleccionamos una submuestra de 1.000 búsquedas para trabajar con la API de Google Books (límite de consulta diaria por usario)\n",
    "* Enviamos la submuestra a la API de Google Books\n",
    "* Procesamos la información obtenida para identificar cuál es libro que más coincide (de un total de hasta 5 candidatos) en función de la aparición de términos en su título, autor y descripción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  ##import requests\n",
    "  ##import pandas as pd\n",
    "  ##from stop_words import get_stop_words\n",
    "  ##import string\n",
    "  ##from textblob import TextBlob\n",
    "  ##import inflection\n",
    "  ##import inflect\n",
    "  ##import re\n",
    "  ##from collections import Counter\n",
    "  ##import time\n",
    "\n",
    "\n",
    "#Cargamos fichero de muestra\n",
    "\n",
    " ##df_words = pd.read_csv('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/queries123sc_sample.csv')\n",
    "df_words = pd.read_csv(os.path.join(base_dir,queries123sc_sample.csv)\n",
    "\n",
    "df_words003 = df_words[['customer','query_spel_corr', 'language', 'country', 'Index']]\n",
    "frames = [df_words003]\n",
    "df_words003 = pd.concat(frames)\n",
    "\n",
    "#Seleccionamos las columnas que nos interesan\n",
    "df_words1 = df_words003[['customer','query_spel_corr', 'language','country', 'Index']]\n",
    "doc_complete = df_words1['query_spel_corr'].tolist()\n",
    "\n",
    "#Limpiamos stop-words en inglés\n",
    "stop = get_stop_words('en')\n",
    "exclude = set(string.punctuation) \n",
    "inf = inflect.engine()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i.encode(\"utf-8\") for i in doc.lower().split() if i not in stop])\n",
    "    \n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    return punc_free\n",
    "    return punc_free\n",
    "\n",
    "def num(word):\n",
    "    if word.isdigit():\n",
    "        return inf.number_to_words(word , andword='')\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "df_words1 = df_words1.iloc[2700:2800]\n",
    "doc_clean = [clean(doc) for doc in doc_complete][2700:2800]  \n",
    "\n",
    "print doc_clean\n",
    "\n",
    "\n",
    "\n",
    "# Definimos función para buscar palabras completas en un texto\n",
    "\n",
    "def findWholeWord(w):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search  \n",
    "\n",
    "\n",
    "# Definimos función para enviar un string y obtener una lista con las palabras en singular y los números convertidos a palabra\n",
    "\n",
    "def words(text): \n",
    "    text_low = text.lower()\n",
    "    text_split = text_low.split()\n",
    "    text_final = []\n",
    "    for word in text_split:\n",
    "        word_sing = inflection.singularize(word)\n",
    "        word_num = num(word_sing)\n",
    "                                        #A veces al singularizar da problemas\n",
    "        word_final = word_num.replace(\"'\", \"\")\n",
    "        text_final.append(word_final)\n",
    "    return text_final\n",
    "\n",
    "\n",
    "# Definimos función para identificar el libro más similar de todos los libros encontrados\n",
    "\n",
    "def best_match(title_list, match_title_list, authors_list, match_authors_list, description_list , match_description_list, word_ocur_list): \n",
    "    match_title_list =[0 if x==\"error\" else x for x in match_title_list]\n",
    "    match_authors_list =[0 if x==\"error\" else x for x in match_authors_list]\n",
    "    match_description_list =[0 if x==\"error\" else x for x in match_description_list]\n",
    "    word_ocur_list =  [0 if x==\"error\" else x for x in word_ocur_list]\n",
    "    \n",
    "    max_title_index = []\n",
    "    max_title = max(match_title_list)\n",
    "    \n",
    "    for j in range(len(match_title_list)):\n",
    "        if max_title == match_title_list[j]:\n",
    "            max_title_index.append(j)\n",
    "            \n",
    "    max_authors_index = []\n",
    "    max_author= max(match_authors_list)\n",
    "    \n",
    "    for j in range(len(match_authors_list)):\n",
    "        if max_author == match_authors_list[j]:\n",
    "            max_authors_index.append(j)\n",
    "            \n",
    "    description_list_2 = []\n",
    "    max_description_index = []\n",
    "    word_ocur_list_2 =  []\n",
    "    max_word_ocur_index = []\n",
    "    author = 0\n",
    "                        #Si el match de autor es menor que 0.5 consideraremos que no busca un autor\n",
    "    if max_author <= 0.5:\n",
    "        for j in range(len(max_title_index)):\n",
    "            description_list_2.append(match_description_list[max_title_index[j]])\n",
    "    else:\n",
    "        author = 1\n",
    "        for j in range(len(max_authors_index)):\n",
    "            description_list_2.append(match_description_list[max_authors_index[j]])\n",
    "    \n",
    "    max_description = max(description_list_2)\n",
    "    \n",
    "    for j in range (len(description_list_2)):\n",
    "        if description_list_2[j] == max_description:\n",
    "            if max_author <= 0.5:\n",
    "                max_description_index.append(max_title_index[j])\n",
    "            else: \n",
    "                max_description_index.append(max_authors_index[j])\n",
    "                \n",
    "    for j in range(len(max_description_index)):\n",
    "        word_ocur_list_2.append(word_ocur_list[max_description_index[j]]) \n",
    "        \n",
    "    max_word_ocur = max(word_ocur_list_2)\n",
    "        \n",
    "    for j in range (len(word_ocur_list_2)):\n",
    "        if word_ocur_list_2[j] == max_word_ocur:\n",
    "            max_word_ocur_index.append(max_description_index[j])\n",
    "    #Comprobamos que al menos uno de los elementos devueltos no tiene match 0  \n",
    "    match_title_final = 0\n",
    "    match_author_final = 0\n",
    "    for j in max_word_ocur_index:\n",
    "        if match_title_list[j] > match_title_final:\n",
    "            match_title_final = match_title_list[j]\n",
    "        if match_authors_list[j] > match_author_final:\n",
    "            match_author_final = match_authors_list[j]\n",
    "        \n",
    "    if match_title_final == 0 and match_author_final == 0:   \n",
    "        return [], author\n",
    "    else: \n",
    "        return max_word_ocur_index, author\n",
    "    \n",
    "\n",
    "\n",
    "# Definimos función de búsqueda de información relacionada con libros contra la API de Google Books\n",
    "\n",
    "def search(value, key):\n",
    "    parms = {\"q\":value, 'key':key}\n",
    "    r = requests.get(url=\"https://www.googleapis.com/books/v1/volumes\", params=parms)\n",
    "    print r\n",
    "    print r.url\n",
    "    rj = r.json()\n",
    "    english = 'en'\n",
    "        #print rj[\"totalItems\"]\n",
    "    j = 0\n",
    "\n",
    "\n",
    "#Lanzamos el proceso de búsqueda y obtención de información de la API de Google Books\n",
    "    \n",
    "    WORDS = Counter(words(value))\n",
    "    \n",
    "    print WORDS\n",
    "    \n",
    "    kind = []\n",
    "    title = []\n",
    "    match_title = []\n",
    "    authors = []\n",
    "    match_authors = []\n",
    "    categories =[]\n",
    "    language = []\n",
    "    description = []\n",
    "    match_description = []\n",
    "    word_ocurr_description = []\n",
    "    \n",
    "    try:\n",
    "        for i in rj[\"items\"]:\n",
    "            lang =   i[\"volumeInfo\"][\"language\"]\n",
    "            if lang == english :\n",
    "                if 'book' in i[\"kind\"]:\n",
    "                    try:\n",
    "                        kind.append(i[\"kind\"])\n",
    "                    except:\n",
    "                        kind.append(\"\")\n",
    "                    try:\n",
    "                        title.append(i[\"volumeInfo\"][\"title\"])\n",
    "                    except:\n",
    "                        title.append(\"\")\n",
    "                    try:\n",
    "                        authors.append(i[\"volumeInfo\"][\"authors\"])\n",
    "                    except:\n",
    "                        authors.append(\"\") \n",
    "                    try:\n",
    "                        categories.append(i[\"volumeInfo\"][\"categories\"])\n",
    "                    except:\n",
    "                        categories.append(\"\")\n",
    "                    try:\n",
    "                        language.append(i[\"volumeInfo\"][\"language\"])\n",
    "                    except:\n",
    "                        language.append(\"\")\n",
    "                    try:\n",
    "                        description.append(i[\"volumeInfo\"][\"description\"])\n",
    "                    except:\n",
    "                        description.append(\"\")\n",
    "            \n",
    "                    j = j + 1\n",
    "            if j > 10:\n",
    "                break   \n",
    "        k = 0\n",
    "       \n",
    "        for k in range(len(title)):\n",
    "            \n",
    "            #Decode utf-8 para quitar el u'word'\n",
    "            try:\n",
    "               \n",
    "                title_list = title[k].decode(encoding='UTF-8',errors='ignore').split()\n",
    "                #quitamos signos de puntuación y stop words y volvemos a codificar en utf-8\n",
    "                title_clean = [clean(word.lower()).encode(encoding='UTF-8',errors='ignore') for word in title_list]\n",
    "                #ponemos en singular las palabras\n",
    "                title_clean_sing = [inflection.singularize(word).encode(encoding='UTF-8',errors='ignore')  for word in title_clean]\n",
    "                #convertimos los números a palabras\n",
    "                title_clean_num = [num(word).encode(encoding='UTF-8',errors='ignore') for word in title_clean_sing]\n",
    "    \n",
    "                #lo juntamos todo en un string, quitamos los vacios generados en la lista\n",
    "                title_str = \" \".join(word for word in title_clean_num if word != \"\")  \n",
    "                match_pct = 0  \n",
    "                match = 0\n",
    "               \n",
    "        \n",
    "                for word in WORDS:\n",
    "                    \n",
    "                    if findWholeWord(word)(title_str):\n",
    "                        \n",
    "                        match = match + 1\n",
    "                    \n",
    "                \n",
    "                \n",
    "                match_pct = round(float(match)/float(len(WORDS)),6)\n",
    "               \n",
    "                match_title.append(match_pct)\n",
    "            except:\n",
    "                match_title.append('error')\n",
    "            \n",
    "        \n",
    "        k = 0\n",
    "    \n",
    "        for item in authors:\n",
    "            \n",
    "            match_pct = 0  \n",
    "            match = 0\n",
    "            \n",
    "            for j in range(len(item)):\n",
    "                try:\n",
    "                    author_list = item[j].decode(encoding='UTF-8',errors='ignore').split()\n",
    "                \n",
    "                    #quitamos signos de puntuación y stop words y volvemos a codificar en utf-8\n",
    "                    author_clean = [clean(word.lower()).encode(encoding='UTF-8',errors='ignore') for word in author_list]\n",
    "                    #ponemos en singular las palabras\n",
    "                    author_clean_sing = [inflection.singularize(word).encode(encoding='UTF-8',errors='ignore')  for word in author_clean]\n",
    "                    #convertimos los números a palabras\n",
    "                    author_clean_num = [num(word).encode(encoding='UTF-8',errors='ignore') for word in author_clean_sing]\n",
    "    \n",
    "                    #lo juntamos todo en un string, quitamos los vacios generados en la lista\n",
    "                    author_str = \" \".join(word for word in author_clean_num if word != \"\")  \n",
    "    \n",
    "                    \n",
    "        \n",
    "                    for word in WORDS:\n",
    "                        \n",
    "                        if findWholeWord(word)(author_str):\n",
    "                        \n",
    "                            match = match + 1\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            match_pct = round(float(match)/float(len(WORDS)),6)\n",
    "            \n",
    "            match_authors.append(match_pct)\n",
    "                  \n",
    "                \n",
    "                \n",
    "              \n",
    "            k = k + 1 \n",
    "      \n",
    "    \n",
    "    \n",
    "        for item in description:\n",
    "            \n",
    "            k = k + 1\n",
    "            if len(item) > 3 and TextBlob(item).detect_language() == 'en':\n",
    "            #Decode utf-8 para quitar el u'word'\n",
    "                try:\n",
    "                    description_list = item.decode(encoding='UTF-8',errors='ignore').split()\n",
    "                    #quitamos signos de puntuación y stop words y volvemos a codificar en utf-8\n",
    "                    description_clean = [clean(word.lower()).encode(encoding='UTF-8',errors='ignore') for word in description_list]\n",
    "                    description_clean_sing = [inflection.singularize(word).encode(encoding='UTF-8',errors='ignore')  for word in description_clean]\n",
    "                    description_clean_num = [num(word).encode(encoding='UTF-8',errors='ignore') for word in description_clean_sing]\n",
    "    #lo juntamos todo en un string, quitamos los vacios generados en la lista\n",
    "                    description_str = \" \".join(word for word in description_clean_num if word != \"\")  \n",
    "                    \n",
    "                   \n",
    "                    match_pct = 0  \n",
    "                    match = 0\n",
    "                    word_ocurr = 0\n",
    "                    \n",
    "                    \n",
    "                    for word in WORDS:\n",
    "                       \n",
    "                        if findWholeWord(word)(description_str):\n",
    "                            match = match + 1\n",
    "                            word_ocurr = word_ocurr + description_clean_num.count(word)\n",
    "                \n",
    "                    match_pct = round(float(match)/float(len(WORDS)), 6)\n",
    "                    \n",
    "                    match_description.append(match_pct)\n",
    "                    word_ocurr_description.append(word_ocurr)\n",
    "                except:\n",
    "                \n",
    "                    match_description.append('error')\n",
    "                    word_ocurr_description.append('error')\n",
    "            else:\n",
    "                match_description.append(0)\n",
    "                word_ocurr_description.append(0)\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "   \n",
    "    return title, match_title, authors, match_authors, description, match_description, word_ocurr_description, categories\n",
    "\n",
    "\n",
    "googleapikey= \"AIzaSyA2Qp-ys1jqQ1Lfp8ZUYA-Onjy4_rrh2AQ\"\n",
    "\n",
    "def search_query(query, call_yn, result, match):\n",
    "    global title_final\n",
    "    global author_final\n",
    "    global description_final\n",
    "    global category_final\n",
    "    global author_yn_final\n",
    "    global index\n",
    "    global query_num\n",
    "    \n",
    "    global title_2\n",
    "    global title_3\n",
    "    global title_4\n",
    "    global title_5\n",
    "            \n",
    "    global author_2\n",
    "    global author_3\n",
    "    global author_4\n",
    "    global author_5\n",
    "            \n",
    "    global description_2\n",
    "    global description_3\n",
    "    global description_4\n",
    "    global description_5\n",
    "            \n",
    "    global category_2\n",
    "    global category_3\n",
    "    global category_4\n",
    "    global category_5\n",
    "            \n",
    "    title_list = []\n",
    "    match_title_list = []\n",
    "    authors_list = []\n",
    "    match_authors_list = []\n",
    "    description_list = []\n",
    "    match_description_list = []\n",
    "    word_ocur_list = []\n",
    "    categories_list = []\n",
    "    \n",
    "  \n",
    "    if call_yn == 1:\n",
    "        \n",
    "        \n",
    "        title_list, match_title_list, authors_list, match_authors_list, description_list , match_description_list, word_ocur_list, categories_list = search(query, googleapikey)\n",
    "        \n",
    "        time.sleep(3)\n",
    "        if match_title_list != []:\n",
    "            index, author_yn = best_match(title_list, match_title_list, authors_list, match_authors_list, description_list , match_description_list, word_ocur_list)\n",
    "        else:\n",
    "            index = []\n",
    "            author_yn = 0\n",
    "            \n",
    "        print title_list\n",
    "        print match_title_list\n",
    "        print match_authors_list\n",
    "        print match_description_list\n",
    "        print word_ocur_list\n",
    "    \n",
    "        \n",
    "        print index\n",
    "    \n",
    "        if index == []:\n",
    "            title_final = \"0\"\n",
    "            author_final.append(\"0\")\n",
    "            description_final.append(\"0\")\n",
    "            category_final.append(\"0\")\n",
    "            author_yn_final.append(0)\n",
    "            \n",
    "            title_2.append(\"0\")\n",
    "            title_3.append(\"0\")\n",
    "            title_4.append(\"0\")\n",
    "            title_5.append(\"0\")\n",
    "            \n",
    "            author_2.append(\"0\")\n",
    "            author_3.append(\"0\")\n",
    "            author_4.append(\"0\")\n",
    "            author_5.append(\"0\")\n",
    "            \n",
    "            description_2.append(\"0\")\n",
    "            description_3.append(\"0\")\n",
    "            description_4.append(\"0\")\n",
    "            description_5.append(\"0\")\n",
    "            \n",
    "            category_2.append(\"0\")\n",
    "            category_3.append(\"0\")\n",
    "            category_4.append(\"0\")\n",
    "            category_5.append(\"0\")\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            title_final = title_list[index[0]].encode(encoding='UTF-8',errors='ignore')\n",
    "            \n",
    "            for item in authors_list[index[0]]:\n",
    "                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "            author_final.append(authors_list[index[0]])\n",
    "            description_final.append(description_list[index[0]].encode(encoding='UTF-8',errors='ignore'))\n",
    "            \n",
    "            for item in categories_list[index[0]]:\n",
    "                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "            category_final.append(categories_list[index[0]])\n",
    "            author_yn_final.append(author_yn)\n",
    "            n = 0\n",
    "            p = 0\n",
    "            \n",
    "            \n",
    "            \n",
    "            for item in match_title_list:\n",
    "                print item\n",
    "                if item >= 0.6 and item != \"error\":\n",
    "                    print index[0]\n",
    "                    if n!= index[0]:\n",
    "                        print n, p\n",
    "                        if p == 0:\n",
    "                            print title_list[n]\n",
    "                            title_2.append(title_list[n].encode(encoding='UTF-8',errors='ignore'))\n",
    "                            description_2.append(description_list[n].encode(encoding='UTF-8',errors='ignore'))\n",
    "                            for item in authors_list[n]:\n",
    "                                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "                            author_2.append(authors_list[n])\n",
    "                            for item in categories_list[n] :\n",
    "                                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "                            category_2.append(categories_list[n])\n",
    "                            \n",
    "                            \n",
    "                        if p == 1:\n",
    "                            title_3.append(title_list[n].encode(encoding='UTF-8',errors='ignore'))\n",
    "                            description_3.append(description_list[n].encode(encoding='UTF-8',errors='ignore'))\n",
    "                            for item in authors_list[n]:\n",
    "                                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "                            author_3.append(authors_list[n])\n",
    "                            for item in categories_list[n] :\n",
    "                                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "                            category_3.append(categories_list[n])\n",
    "                            \n",
    "                        if p == 2:\n",
    "                            title_4.append(title_list[n].encode(encoding='UTF-8',errors='ignore'))\n",
    "                            description_4.append(description_list[n].encode(encoding='UTF-8',errors='ignore'))\n",
    "                            for item in authors_list[n]:\n",
    "                                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "                            author_4.append(authors_list[n])\n",
    "                            for item in categories_list[n] :\n",
    "                                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "                            category_4.append(categories_list[n])\n",
    "                            \n",
    "                        if p == 3:\n",
    "                            title_5.append(title_list[n].encode(encoding='UTF-8',errors='ignore'))\n",
    "                            description_5.append(description_list[n].encode(encoding='UTF-8',errors='ignore'))\n",
    "                            for item in authors_list[n]:\n",
    "                                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "                            author_5.append(authors_list[n])\n",
    "                            for item in categories_list[n] :\n",
    "                                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "                            category_5.append(categories_list[n])\n",
    "                            \n",
    "                            \n",
    "                    \n",
    "                        if p > 3:\n",
    "                            \n",
    "                            pass\n",
    "                        p = p + 1\n",
    "                n = n +1\n",
    "            if p == 0:\n",
    "                print 'P acaba 0'\n",
    "                title_2.append(\"0\")\n",
    "                title_3.append(\"0\")\n",
    "                title_4.append(\"0\")\n",
    "                title_5.append(\"0\")\n",
    "                category_2.append(\"0\")\n",
    "                category_3.append(\"0\")\n",
    "                category_4.append(\"0\")\n",
    "                category_5.append(\"0\")\n",
    "                description_2.append(\"0\")\n",
    "                description_3.append(\"0\")\n",
    "                description_4.append(\"0\")\n",
    "                description_5.append(\"0\")\n",
    "                author_2.append(\"0\")\n",
    "                author_3.append(\"0\")\n",
    "                author_4.append(\"0\")\n",
    "                author_5.append(\"0\")\n",
    "                \n",
    "                print title_2\n",
    "            if p == 1:\n",
    "                    \n",
    "                print 'P acaba 1'\n",
    "                title_3.append(\"0\")\n",
    "                title_4.append(\"0\")\n",
    "                title_5.append(\"0\")\n",
    "                    \n",
    "                category_3.append(\"0\")\n",
    "                category_4.append(\"0\")\n",
    "                category_5.append(\"0\")\n",
    "                    \n",
    "                description_3.append(\"0\")\n",
    "                description_4.append(\"0\")\n",
    "                description_5.append(\"0\")\n",
    "                \n",
    "                author_3.append(\"0\")\n",
    "                author_4.append(\"0\")\n",
    "                author_5.append(\"0\")\n",
    "            if p == 2:\n",
    "                    \n",
    "                print 'P acaba 2'   \n",
    "                title_4.append(\"0\")\n",
    "                title_5.append(\"0\")\n",
    "                   \n",
    "                category_4.append(\"0\")\n",
    "                category_5.append(\"0\")\n",
    "                    \n",
    "                description_4.append(\"0\")\n",
    "                description_5.append(\"0\")\n",
    "                author_4.append(\"0\")\n",
    "                author_5.append(\"0\")\n",
    "            if p == 3:\n",
    "                    \n",
    "                print 'P acaba 3'                        \n",
    "                title_5.append(\"0\")\n",
    "                category_5.append(\"0\")\n",
    "                    \n",
    "                description_5.append(\"0\")\n",
    "            \n",
    "                author_5.append(\"0\")\n",
    "                \n",
    "    else:  \n",
    "        pass    \n",
    "    \n",
    "    if result == 1 and match == 1:\n",
    "        \n",
    "        \n",
    "        return title_final \n",
    "        \n",
    "    if result == 1 and match == 2:\n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        print title_2\n",
    "        \n",
    "        \n",
    "        print 'Title 2:' , title_2[k]\n",
    "        return title_2[k]\n",
    "    \n",
    "    if result == 1 and match == 3:\n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        return title_3[k]\n",
    "        \n",
    "            \n",
    "    if result == 1 and match == 4:\n",
    "        \n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        \n",
    "        if k < len(title_4):\n",
    "            \n",
    "            return title_4[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    if result == 1 and match == 5:\n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        \n",
    "        if k < len(title_5):\n",
    "            \n",
    "            return title_5[k]\n",
    "        else:\n",
    "            return \"0\"    \n",
    "        \n",
    "    if result == 2 and match == 1:\n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        return author_final [k]\n",
    "    \n",
    "    \n",
    "    if result == 2 and match == 2:\n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        if k < len(author_2):\n",
    "            \n",
    "            return author_2[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    \n",
    "    \n",
    "    if result == 2 and match == 3:\n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        if k < len(author_3):\n",
    "            \n",
    "            return author_3[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    \n",
    "    if result == 2 and match == 4:\n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        if k < len(author_4):\n",
    "            \n",
    "            return author_4[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "        \n",
    "    \n",
    "    if result == 2 and match == 5:\n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        if k < len(author_5):\n",
    "            \n",
    "            return author_5[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    \n",
    "    \n",
    "    if result == 3 and match == 1:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0] - query_inicial\n",
    "        return description_final[k]\n",
    "    \n",
    "    if result == 3 and match == 2:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0] - query_inicial\n",
    "        if k < len(description_2):\n",
    "            \n",
    "            return description_2[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    \n",
    "    if result == 3 and match == 3:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0] - query_inicial\n",
    "        if k < len(description_3):\n",
    "            \n",
    "            return description_3[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    if result == 3 and match == 4:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0] - query_inicial\n",
    "        if k < len(description_4):\n",
    "            \n",
    "            return description_4[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    if result == 3 and match == 5:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0] - query_inicial\n",
    "        if k < len(description_5):\n",
    "            \n",
    "            return description_5[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "        \n",
    "        \n",
    "    if result == 4 and match == 1:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0]- query_inicial\n",
    "        return category_final[k]\n",
    "    \n",
    "    if result == 4 and match == 2:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0]- query_inicial\n",
    "        if k < len(category_2):\n",
    "            \n",
    "            return category_2[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    \n",
    "    if result == 4 and match == 3:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0]- query_inicial\n",
    "        if k < len(category_3):\n",
    "            \n",
    "            return category_3[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    \n",
    "    if result == 4 and match == 4:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0]- query_inicial\n",
    "        if k < len(category_4):\n",
    "            \n",
    "            return category_4[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    \n",
    "    if result == 4 and match == 5:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0]- query_inicial\n",
    "        if k < len(category_5):\n",
    "            \n",
    "            return category_5[k]\n",
    "        else:\n",
    "            return \"0\"        \n",
    "            \n",
    "    \n",
    "        \n",
    "    if result == 5 and match == 1:\n",
    "        query_num = query_num + 1\n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0]- query_inicial\n",
    "        return author_yn_final[k]\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "    if result != 1 and result != 2 and result != 3 and result != 4 and result != 5:\n",
    "        print 'Error not valid result'\n",
    "        return []\n",
    "        \n",
    "\n",
    "\n",
    "print df_words1.head()\n",
    "\n",
    "\n",
    "\n",
    "index = []\n",
    "title_final = []\n",
    "author_final   = []\n",
    "description_final  = []\n",
    "category_final  = []\n",
    "author_yn_final  = []\n",
    "\n",
    "\n",
    "index = []\n",
    "title_final = \"\"\n",
    "author_final   = []\n",
    "description_final  = []\n",
    "category_final  = []\n",
    "author_yn_final  = []\n",
    "\n",
    "\n",
    "\n",
    "title_2 = []\n",
    "title_3 = []\n",
    "title_4 = []\n",
    "title_5 = []\n",
    "            \n",
    "author_2 = []\n",
    "author_3 = []\n",
    "author_4 = []\n",
    "author_5 = []\n",
    "\n",
    "description_2 = []\n",
    "description_3 = []\n",
    "description_4 = []\n",
    "description_5 = []\n",
    "            \n",
    "category_2 = []\n",
    "category_3 = []\n",
    "category_4 = []\n",
    "category_5 = []\n",
    "\n",
    "global query_inicial\n",
    "\n",
    "query_inicial = 2700\n",
    "\n",
    "\n",
    "df_words1[\"Title_1\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,1,1,1))\n",
    "\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Title_2\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,1,2))\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Title_3\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,1,3))\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Title_4\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,1,4))\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Title_5\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,1,5))\n",
    "\n",
    "\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Author_1\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,2,1))\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Author_2\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,2,2))\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Author_3\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,2,3))\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Author_4\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,2,4))\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Author_5\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,2,5))\n",
    "\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Description_1\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,3,1))\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Description_2\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,3,2))\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Description_3\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,3,3))\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Description_4\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,3,4))\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Description_5\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,3,5))\n",
    "\n",
    "\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Category_1\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,4,1))\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Category_2\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,4,2))\n",
    "\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Category_3\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,4,3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Category_4\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,4,4))\n",
    "\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Category_5\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,4,5))\n",
    "\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Author_search_1\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,5,1))\n",
    "\n",
    "          \n",
    "\n",
    "  ##df_words1.to_csv('C:/Users/Paqui/Programas Python/Capstone project/df_queries_books_sample_09.csv')\n",
    "df_words1.to_csv(os.path.join(base_dir,df_queries_books_sample_09.csv))\n",
    "                       \n",
    "print 'FINISHED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
