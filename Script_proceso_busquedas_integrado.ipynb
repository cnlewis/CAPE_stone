{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPSTONE PROJECT\n",
    "\n",
    "#Autores: Patricia Araguz, Cary Lewis, Aina Pascual y Enrique Rodriguez.\n",
    "#Última modificación: 18 Junio 2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named PyDictionary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6978eabbf086>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mPyDictionary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPyDictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named PyDictionary"
     ]
    }
   ],
   "source": [
    "#Cargamos todas las librerías que utilizaremos\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from textblob import TextBlob\n",
    "import langdetect as lang\n",
    "import pickle\n",
    "from PyDictionary import PyDictionary\n",
    "import sys\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import enchant as ench\n",
    "from stop_words import get_stop_words\n",
    "import string\n",
    "import inflection\n",
    "import inflect\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from pytagcloud import create_tag_image, make_tags\n",
    "from pytagcloud.lang.counter import get_tag_counts\n",
    "\n",
    "#Definimos variables de entorno\n",
    "\n",
    "base_dir = r'D:\\Capstone-Wordreader\\Data' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga, exploración, limpieza y selección de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos los procesos necesarios para conocer nuestros datos, normalizar el texto y detectar el idioma en el que ha sido escrita cada búsqueda:\n",
    "\n",
    "* Carga de datos y exloración general. Eliminamos duplicados por usuario.\n",
    "* Limpieza de datos: eliminamos signos de puntuación, espacios al principio y/o final de la frase, celdas vacías o celdas que contienen solo números.\n",
    "* Detección del idioma: librerías _textblob_ y _langdetect_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# es necesario instalar las librerías :\n",
    "#   textblob\n",
    "#   langdetect\n",
    "\n",
    "#cargamos librerías\n",
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import langdetect as lang\n",
    "import os\n",
    "\n",
    "\n",
    "#Definimos variables de entorno\n",
    "  # base_dir = r'D:\\Capstone-Wordreader\\Data'\n",
    "base_dir = r'/Data/'\n",
    "\n",
    "\n",
    "#cargamos fichero y seleccionamos campos\n",
    "\n",
    "##df = pd.read_csv('C:/Users/Paqui/Programas Python/Capstone project/queries003.csv')\n",
    "df = pd.read_csv(os.path.join(base_dir,'queries123_sample.csv'))\n",
    "df1 = df[['customer','country','query']]\n",
    "\n",
    "\n",
    "#contamos el nÃºmero de búsquedas por usuarios\n",
    "\n",
    "df_busc_user= df1.groupby('customer').count()\n",
    "media_busquedas= df_busc_user.mean()\n",
    "media_busquedas\n",
    "\n",
    "#contamos el nÃºmero de diferentes búsquedas por usuarios\n",
    "\n",
    "df_difbusc_user= df1.groupby('customer')['query'].nunique()\n",
    "media_difbusquedas= df_difbusc_user.mean()\n",
    "media_difbusquedas\n",
    "\n",
    "#Creamos un dataframe agrupado por customer y query\n",
    "\n",
    "df2 = pd.DataFrame({'count' : df1.groupby( [ \"customer\", \"country\", \"query\"] ).size()}).reset_index()\n",
    "print df2.head()\n",
    "\n",
    "\n",
    "#Contamos con cuantas querys nos hemos quedado\n",
    "\n",
    "print \"Querys no unificadas\", df1['query'].count()\n",
    "print \"Querys unificadas\",    df2['query'].count()\n",
    "\n",
    "\n",
    "#Eliminamos los special caracters, menos los espacios y los apóstrofes\n",
    "#Con la funciÃ³n strip nos quitamos los espacios al inicio y al final de la línea\n",
    "\n",
    "df2['query_2'] = df2['query'].map(lambda x: re.sub(r\"[^\\w' ]\", '', x))\n",
    "df2['query_2'] = df2['query_2'].map(lambda x: re.sub(r\"_\", '', x))\n",
    "df2['query_2'] = df2['query_2'].str.strip()\n",
    "df2.head()\n",
    "print df2.head()\n",
    "\n",
    "\n",
    "#Eliminamos las querys que contienen solo números con espocios o solo espacios y contamos cuantas tenemos\n",
    "\n",
    "df2['query_2'] = df2['query_2'].map(lambda x: re.sub(r\"^[0-9 ]+$\", '', x))\n",
    "print df2.head()\n",
    "print \"Querys no numÃ©ricas\",    df2['query_2'].count()\n",
    "\n",
    "\n",
    "#Eliminamos las filas con texto vacÃ­o\n",
    "df3 = df2[df2['query_2'] != \"\"]\n",
    "print \"Querys no vacÃ­as\",    df3['query_2'].count()\n",
    "print df3.head()\n",
    "\n",
    "\n",
    "#Si la query tiene menos de 3 carácteres entonces usamos el langdetect porque el otro da error\n",
    "\n",
    "def detect_lang(query):\n",
    "    if len(query) > 3:\n",
    "        return TextBlob(query).detect_language()\n",
    "    else:\n",
    "        return lang.detect(query)\n",
    "\n",
    "df3['language'] = df3['query_2'].apply(detect_lang)\n",
    "df3.head()\n",
    "\n",
    "##df3.to_csv('C:/Users/Paqui/Programas Python/Capstone project/queries003_corrected.csv')\n",
    "df3.to_csv(os.path.join(base_dir,'queries123_sample_corrected.csv'))\n",
    "\n",
    "print 'FINISHED'\n",
    "\"\"\"\n",
    "for i in xrange(312112,315655):\n",
    "    text = df3.iloc[i,3]\n",
    "    print text\n",
    "    language = lang.detect(text)\n",
    "    print i, \": \", language\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización de búsquedas en inglés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el contador de palabras existentes y el string con las palabras erróneas pendientes de corregir:\n",
    "\n",
    "* Leemos el fichero  limpiado  y con el idioma asignado y nos quedamos con los campos que nos interesan.\n",
    "* Creamos un variable string juntando todas las queries. Ponemos en minúsculas todas las palabras. \n",
    "* Creamos un contador de palabras WORDS utilizando la variable string anterior. Si hemos procesado ya otros ficheros juntamos todos los contadores de palabras de cada uno de ellos en el contador WORDS_EXIST para no volver a chequear de nuevo las palabras ya comprobadas anteriormente.\n",
    "* Seleccionamos el inglés como idioma para la libreria Enchant y nltk\n",
    "* Comprobamos usando varios métodos si cada palabra del contador WORDS existe o no: libreria enchant , libreria nltk, Diccionario python. Si la palabra ya ha sido chequeada anteriormente (aparece en el contador WORDS_EXIST) no la volvemos a chequear.\n",
    "* Generamos un string  de palabras erróneas pendientes de corregir (palabras no encontradas utilizando los 3 métodos anteriores).\n",
    "* Borramos las palabras erróneas pendientes de corregir del contador WORDS.\n",
    "* Guardamos el contador WORDS con las palabras correctas y el string errors con las palabras pendiente de corregir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  # es necesario instalar las librerías :\n",
    "#   pyenchant\n",
    "#   PyDictionary\n",
    "#   nltk.download()\n",
    "\n",
    "\n",
    "#cargamos librerías\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import enchant as ench\n",
    "import pickle\n",
    "import nltk\n",
    "from PyDictionary import PyDictionary\n",
    "import timeit\n",
    "import os\n",
    "\n",
    "\n",
    "#Definimos variables de entorno\n",
    "  # base_dir = r'D:\\Capstone-Wordreader\\Data'\n",
    "base_dir = r'/Data/'\n",
    "\n",
    "\n",
    "dictionary=PyDictionary()\n",
    "\n",
    "#Leemos el fichero corregido y con el idioma asignado y nos quedamos con los campos que nos interesan\n",
    "df_words = pd.read_csv(os.path.join(base_dir,'queries123_sample_corrected.csv'))\n",
    "df_words1 = df_words[['customer','query_2', 'language']]\n",
    "\n",
    "\n",
    "#Seleccionamos las queries en inglés y vemos cuantas hay\n",
    "df_words2 = df_words1[df_words1['language'] == 'en']\n",
    "df_words2.head()\n",
    "print \"Querys corregidas en todos los idiomas \",    df_words1['query_2'].count()\n",
    "print \"Querys en inglÃ©s \",    df_words2['query_2'].count()\n",
    "\n",
    "#Creamos un variable string juntando todas las queries. Ponemos en minúsculas todas las palabras\n",
    "size =  int(df_words2['query_2'].count() -1)\n",
    "print size\n",
    "df_words3 = df_words2.iloc[0:size, 1]\n",
    "string = pd.DataFrame(' '.join(df_words3.tolist()), columns=['query_2'], index=[0]).iloc[0,0]\n",
    "string = string.lower()\n",
    "print string\n",
    "\n",
    "#Definimos el idioma que vamos a usar para la libreria Enchant y el vocabulario inglés de la libreria nltk\n",
    "diction = ench.Dict(\"en_UK\")\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "\n",
    "#Definimos la función que nos permitirá buscar una palabra en el diccionario de Python y ver si la encuentra (se puede buscar en el normal o usando la versiÃ³n google)\n",
    "\n",
    "def dictionary_meaning(word):\n",
    "    try:\n",
    "        meaning_1 = dictionary.meaning(word) \n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        meaning_2 = dictionary.googlemeaning(word) \n",
    "         \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if meaning_1 == None and meaning_2 == None:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "\n",
    "#Comprobamos usando varios métodos si la palabra existe o no:\n",
    "#    libreria enchant \n",
    "#    libreria nltk\n",
    "#    Diccionario python\n",
    "# En caso que no exista en ninguno no la adjuntaremos al contador final de palabras\n",
    "# Si la palabra ya ha sido chequeada entonces la adjuntmmos o no directamente\n",
    "\n",
    "WORDS = Counter(words(string))\n",
    "\n",
    "print WORDS\n",
    "string_errors = \"\"\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "\n",
    "df_words = pd.read_csv(os.path.join(base_dir,'queries123_sample_corrected.csv'))\n",
    "\n",
    "  ##inputfile_001 = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/counter', 'rb')\n",
    "inputfile_001 = open(os.path.join(base_dir,'counter'), 'rb')\n",
    "\n",
    "WORDS_exist001 = Counter (pickle.load(inputfile_001))\n",
    "\n",
    "  ##inputfile_002 = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/counter_002', 'rb')\n",
    "inputfile_002 = open(os.path.join(base_dir,'counter_002'), 'rb')\n",
    "\n",
    "WORDS_exist002 = Counter (pickle.load(inputfile_002))\n",
    "\n",
    "WORDS_exist = WORDS_exist001 + WORDS_exist002\n",
    "\n",
    "# Primero miramos si la palabara ya está en el diccionario procesado en pasos anteriores, si no\n",
    "# Comprobamos usando varios métodos si la palabra existe o no:\n",
    "#     libreria enchant \n",
    "#     libreria nltk\n",
    "#     Diccionario python\n",
    "# En caso que no exista en ninguno no la adjuntaremos al contador final de palabras\n",
    "# Si la palabra ya ha sido chequeada entonces la adjuntmmos o no directamente\n",
    "\n",
    "\n",
    "for word in WORDS:\n",
    "    if word in WORDS_exist.keys():\n",
    "        pass\n",
    "    else:\n",
    "        if diction.check(word) == True:\n",
    "            pass\n",
    "        else:\n",
    "            if word in english_vocab:\n",
    "                pass          \n",
    "            else: \n",
    "                if dictionary_meaning(word) == True: \n",
    "                    pass\n",
    "                else:\n",
    "                    string_errors = string_errors + \" \" + word\n",
    "               \n",
    "errors = string_errors.split()\n",
    "print errors\n",
    "stop_time = timeit.default_timer()    \n",
    "for word in errors:\n",
    "    del WORDS[word]\n",
    "    \n",
    "stop_time = timeit.default_timer()\n",
    "\n",
    "print'Tiempo de ejecución',  stop_time - start_time\n",
    "\n",
    "\n",
    "# Ahora creamos el contador con todas las palabras correctas seleccionadas que nos servirá para corregir el spelling\n",
    "\n",
    "print \"WORDS FINAL\", WORDS\n",
    "\n",
    "WORDS\n",
    "#with open('C:/Users/Paqui/Programas Python/Capstone project/counter_003', 'wb') as outputfile:\n",
    "#   pickle.dump(WORDS , outputfile)\n",
    "with open(os.path.join(base_dir,'counter_003'), 'wb') as outputfile:\n",
    "    pickle.dump(WORDS , outputfile)\n",
    "\n",
    "    \n",
    "errors\n",
    "#with open('C:/Users/Paqui/Programas Python/Capstone project/errors003_counter', 'wb') as outputfile:\n",
    "#    pickle.dump(errors , outputfile)\n",
    "with open(os.path.join(base_dir,'errors_003_counter'), 'wb') as outputfile:\n",
    "    pickle.dump(errors , outputfile)\n",
    "\n",
    "    \n",
    "\n",
    "print 'FINISHED'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el módulo con  las funciones de corrección de palabras:\n",
    "* Creamos un contador de palabras agregando los contadores WORDS generados en cada uno de los archivos procesados\n",
    "* Definimos las funciones de búsqueda y corrección de palabra/s errónea/s\n",
    "* Primero probamos  correcciones básicas de la palabra (eliminar letras, agregar letras, transponer letras, separar la palabra en dos etc). Buscamos las correcciones básicas en el contador de palabras correctas generado con todos los ficheros y seleccionamos la que tiene mayor probabilidad (aparece un mayor número de veces). \n",
    "* En caso de que la correcciones básicas no funcionen (no encuentre ninguna palabra existente correspondiente) utilizamos las sugerencias de la librería enchant y seleccionamos la que tiene mayores probabilidades (aparece un mayor número de veces en el contador de palabras correctas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cargamos librerías\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import numpy as np\n",
    "import enchant as ench\n",
    "\n",
    "#Definimos variables de entorno\n",
    "  # base_dir = r'D:\\Capstone-Wordreader\\Data'\n",
    "base_dir = r'/Data/'\n",
    "\n",
    "\n",
    "diction = ench.Dict(\"en_UK\")\n",
    "\n",
    "#Abrimos los  contadores de palabras procesados anteriormente para cada uno de los ficheros\n",
    "\n",
    "  #inputfile_001 = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/counter', 'rb')\n",
    "  #inputfile_002 = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/counter_002', 'rb')\n",
    "  #inputfile_003 = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/counter_003', 'rb')\n",
    "\n",
    "inputfile_001 = open(os.path.join(base_dir,'counter'), 'rb')\n",
    "inputfile_002 = open(os.path.join(base_dir,'counter_002'), 'rb')\n",
    "inputfile_003 = open(os.path.join(base_dir,'counter_003'), 'rb')\n",
    "\n",
    "WORDS_001 = Counter (pickle.load(inputfile_001))\n",
    "WORDS_002 = Counter (pickle.load(inputfile_002))\n",
    "WORDS_003 = Counter (pickle.load(inputfile_003))\n",
    "\n",
    "\"\"\"\n",
    "Creamos un contador sumando los contadores individuales de cada archivo\n",
    "\"\"\"\n",
    "\n",
    "WORDS = WORDS_001 + WORDS_002 + WORDS_003\n",
    "\n",
    "\n",
    "print  'Number of different words: ' ,len(WORDS.keys())\n",
    "print  'Sum of values of words:', sum(WORDS.values())\n",
    "\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`. No dividimos por el número de palabras de esta forma\"\n",
    "    \"nos evitamos probabilidades cercanas a 0\"\n",
    "    return WORDS[word]\n",
    " \n",
    "def correction(word): \n",
    "    \"We transform the word to lower case\"\n",
    "    word = word.lower()\n",
    "    \"Most probable spelling correction for word.\"\n",
    "    corrected_word1 = max(candidates(word), key=P)\n",
    "    \"Probability of the word.\"\n",
    "    p1 = P(corrected_word1)\n",
    "    \n",
    "    \"Most probable spelling correction for the word splitting subset.\"\n",
    "    corrected_word2 = max_split(word)\n",
    "    \n",
    "    \"If word was not corrected using the basic corrector we use enchant suggestions and we\"\n",
    "    \"select the one with more probability to appear in our text\"\n",
    "    if corrected_word1 == word:\n",
    "        suggestions = diction.suggest(word)\n",
    "        p_aux = 0\n",
    "        p_sug = 0\n",
    "        word_sug = \"\"\n",
    "        for item  in suggestions:\n",
    "            p_aux = P(item)\n",
    "            if p_aux > p_sug:\n",
    "                p_sug = p_aux\n",
    "                word_sug = item\n",
    "        \"We select depending on probability the corrected word or the two words form the splitting subset\"\n",
    "        if corrected_word2[1] > p_sug:\n",
    "            return corrected_word2[0][0] + \" \" + corrected_word2[0][1]\n",
    "        else:\n",
    "            if p_sug > 0:\n",
    "                return word_sug\n",
    "            else:\n",
    "                return word\n",
    "    \"If word corrected with the basic corrector then we return the word\"   \n",
    "    if corrected_word1 != word:\n",
    "        \"We select depending on probability the corrected word or the two words form the splitting subset\"\n",
    "        if corrected_word2[1] > p1:\n",
    "            return corrected_word2[0][0] + \" \" + corrected_word2[0][1]\n",
    "        else:\n",
    "            return corrected_word1\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "   \n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    \n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"All word splits.\"\n",
    "    \n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    \n",
    "  \n",
    "    return splits\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "\n",
    "\n",
    "def max_split(word):\n",
    "    \"All word splits.\"\n",
    "    words_splits = splits(word)\n",
    "    \n",
    "    p = np.zeros(len(words_splits))\n",
    "    \n",
    "    \"Calculating the sum of probabilities of all word splits, the two words must have\" \n",
    "    \"probability bigger than 0 (appear at least one time in the words dictionry\"\n",
    "    \"The words must have a length bigger than one to avoid single letters\"\n",
    "    i = 0\n",
    "    for word in words_splits:\n",
    "        p_0 = P(word[0])\n",
    "        p_1 = P(word[1])\n",
    "        if (p_0 > 0 and len(word[0] )> 1)  and (p_1 >0 and len(word[1])>1):\n",
    "            p[i] = p_0 + p_1\n",
    "        else: \n",
    "            p[i] = 0\n",
    "       \n",
    "        i = i + 1\n",
    "  \n",
    "    j = p.argmax()\n",
    "\n",
    "     \n",
    "    return (words_splits[j], p[j])\n",
    "\n",
    "print correction('CHIDREN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el fichero limpiado y  corregimos palabras erróneas utilizando el string errors y el módulo de corrección de palabras de los pasos anteriores:\n",
    "\n",
    "* Insertamos el módulo de corrección de palabras _P02B_Spelling_corrector6_.\n",
    "* Cargamos el string de palabras erróneas  del fichero generado con el módulo _P02A2_Spelling_text_builder3_.\n",
    "* Para cada una de las palabras erróneas llamamos al módulo de corrección de palabras P02B.\n",
    "* Creamos un dataframe agrupado por customer y query para eliminar duplicados una vez corregido.\n",
    "* Hacemos el recuento de búsquedas una vez corregidas y eliminados los duplicados.\n",
    "* Guardamos el fichero con las palabras corregidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named P02B_Spelling_corrector6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c5ff70f98dca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:/Users/Paqui/Programas Python/Capstone project'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mP02B_Spelling_corrector6\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspelling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named P02B_Spelling_corrector6"
     ]
    }
   ],
   "source": [
    "# cargamos librerías\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "#Definimos variables de entorno\n",
    "  # base_dir = r'D:\\Capstone-Wordreader\\Data'\n",
    "base_dir = r'/Data/'\n",
    "\n",
    "  ##sys.path.append('C:/Users/Paqui/Programas Python/Capstone project')\n",
    "sys.path.append(base_dir)\n",
    "\n",
    "import P02B_Spelling_corrector as spelling\n",
    "\n",
    "  ##df = pd.read_csv('C:/Users/Paqui/Programas Python/Capstone project/queries001.csv')\n",
    "df = pd.read_csv(os.path.join(base_dir,'queries123_sample_corrected.csv'))    \n",
    "\n",
    "df1 = df[['customer','query_2', 'language', 'country']]\n",
    "\n",
    "df1.head()\n",
    "\n",
    "print 'Queries con todos los idiomas:',df1['query_2'].count()\n",
    "\n",
    "df_english = df1[df1.language == 'en']\n",
    "print df_english.head()\n",
    "\n",
    "print 'Queries con idioma inglés :',df_english['query_2'].count()\n",
    "\n",
    "  ##inputfile = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/errors001_counter', 'rb')\n",
    "inputfile = open(os.path.join(base_dir,'errors001_counter'), 'rb')\n",
    "    \n",
    "errors = pickle.load(inputfile)\n",
    "\n",
    "errors_str = \" \".join(str(x) for x in errors)\n",
    "\n",
    "print errors_str\n",
    "\n",
    "def correct_spelling(query):\n",
    "    words = query.split()\n",
    "    query_2 = \"\"\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        corrected_word = \"\"\n",
    "        single_word = \" \" + word.lower() + \" \" \n",
    "        \"We want to match the whole single  word in the words errors string\"\n",
    "        if single_word in errors_str:\n",
    "            corrected_word = spelling.correction(word.lower())\n",
    "            query_2 = query_2 + \" \" + corrected_word\n",
    "        else: \n",
    "            query_2 = query_2 + \" \" + word\n",
    "    \n",
    "    return query_2\n",
    "\n",
    "df_english['query_spel_corr'] = df_english['query_2'].apply(lambda query: correct_spelling(query))\n",
    "\n",
    "\n",
    "# Creamos un dataframe agrupado por customer y query para eliminar duplicados una vez corregido\n",
    "\n",
    "df_english_corr = pd.DataFrame({'count' : df_english.groupby( [ \"customer\", \"country\", \"query_spel_corr\", \"language\"] ).size()}).reset_index()\n",
    "\n",
    "\n",
    "# Contamos con cuantas querys nos hemos quedado\n",
    "\n",
    "print \"Querys corregidas no unificadas\", df_english['query_spel_corr'].count()\n",
    "print \"Querys coregidas  unificadas\",    df_english_corr['query_spel_corr'].count()\n",
    "\n",
    "\n",
    "  ##df_english.to_csv('C:/Users/Paqui/Programas Python/Capstone project/queries001_spell_corrected_comparison.csv')\n",
    "  ##df_english_corr.to_csv('C:/Users/Paqui/Programas Python/Capstone project/queries001_spell_corrected.csv')\n",
    "df_english.to_csv(os.path.join(base_dir,'queries001_spell_corrected_comparison.csv'))\n",
    "df_english_corr.to_csv(os.path.join(base_dir,'queries001_spell_corrected.csv'))\n",
    "\n",
    "\n",
    "print 'FINISHED'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selección de muestra random de 20.000 registros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos una muestra de 20.000 búsquedas del fichero de búsquedas normalizadas corregidas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4d1b61146d55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Paqui/Programas Python/Capstone project/Counter copy/queries001_spell_corrected.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Paqui/Programas Python/Capstone project/Counter copy/queries002_spell_corrected.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Paqui/Programas Python/Capstone project/Counter copy/queries003_spell_corrected.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#cargamos librerías\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#Definimos variables de entorno\n",
    "  # base_dir = r'D:\\Capstone-Wordreader\\Data'\n",
    "base_dir = r'/Data/'\n",
    "\n",
    "\n",
    "#cargamos ficheros y seleccionamos campos\n",
    "\n",
    "df1 = pd.read_csv(os.path.join(base_dir,'queries001_spell_corrected.csv'))\n",
    "df2 = pd.read_csv(os.path.join(base_dir,'queries002_spell_corrected.csv'))\n",
    "df3 = pd.read_csv(os.path.join(base_dir,'queries003_spell_corrected.csv'))\n",
    "\n",
    "df3 = df3.iloc[0:3000]\n",
    "\n",
    "frames = [df1, df2, df3]\n",
    "\n",
    "df= pd.concat(frames)\n",
    "\n",
    "df_sample = df.sample(20000)\n",
    "\n",
    "  ##df_sample.to_csv('C:/Users/Paqui/Programas Python/Capstone project/queries123sc_sample.csv')\n",
    "df_sample.to_csv(os.path.join(base_dir,'queries123sc_sample'.csv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descripción de datos del total y la muestra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos un análisis descriptivo de los términos más utilizados en la muestra de búsquedas y generamos gráficos para:\n",
    "* Comparar la distribución de búsquedas por país del fichero total en relación al fichero de muestra.\n",
    "* Comparar la distribución de términos de la muestra respecto a la distribución de término del fichero completo (de búquedas con palabras corregidas).\n",
    "* Generar la nube de palabras con los términos más utilizados.\n",
    "* Identificar los \"n\" primeros bigramas en funció de su frecuencia de aparición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# es necesario instalar las librerías :\n",
    "#   stop_words\n",
    "#   inflection\n",
    "#   inflect\n",
    "#   Pygame\n",
    "#   pytagcloud\n",
    "#   simplejson\n",
    "\n",
    "\n",
    "#cargamos librerías\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "import nltk\n",
    "import requests\n",
    "from textblob import TextBlob\n",
    "import inflection\n",
    "import inflect\n",
    "import time\n",
    "from pytagcloud import create_tag_image, make_tags\n",
    "from pytagcloud.lang.counter import get_tag_counts\n",
    "\n",
    "\n",
    "#Definimos variables de entorno\n",
    "  # base_dir = r'D:\\Capstone-Wordreader\\Data'\n",
    "base_dir = r'/Data/'\n",
    "\n",
    "\n",
    "# Importamos los CSVs y los unimos\n",
    "\n",
    "df_words = pd.read_csv(os.path.join(base_dir,'queries001_spell_corrected.csv'))\n",
    "df_words001 = df_words[['customer','query_spel_corr', 'language', 'country']]\n",
    "\n",
    "df_words = pd.read_csv(os.path.join(base_dir,'queries002_spell_corrected.csv'))\n",
    "df_words002 = df_words[['customer','query_spel_corr', 'language', 'country']]\n",
    "\n",
    "df_words = pd.read_csv(os.path.join(base_dir,'queries003_spell_corrected.csv'))\n",
    "df_words003 = df_words[['customer','query_spel_corr', 'language', 'country']]\n",
    "\n",
    "                       \n",
    "frames = [df_words001, df_words002, df_words003]\n",
    "\n",
    "df_words1 = pd.concat(frames)\n",
    "\n",
    "\n",
    "\n",
    "# Seleccionamos las queries en inglés y vemos cuantas hay\n",
    "\n",
    "df_words2 = df_words1[df_words1['language'] == 'en']\n",
    "\n",
    "df_words2.head()\n",
    "print \"Querys en inglés \",    df_words2['query_spel_corr'].count()\n",
    "\n",
    "\n",
    "#Definimos la función para poner en minúsculas, quitar  las stop words, puntutuación y plurales \n",
    "\n",
    "stop = get_stop_words('en') # create English stop words list\n",
    "\n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "p_stemmer = PorterStemmer() # Create p_stemmer of class PorterStemmer\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    #print punc_free\n",
    "    number_free = ''.join(ch for ch in punc_free if nltk.pos_tag([ch ])[0][1] != \"CD\")\n",
    "    #print number_free\n",
    "    normalized = \" \".join(p_stemmer.stem(word) for word in number_free.split())\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "\n",
    "def words_frequencies(doc_list):\n",
    "    #Creamos un variable string juntando todas las queries. Ponemos en minúsculas todas las \n",
    "    #palabras\n",
    "\n",
    "    doc_clean  = [clean(doc) + \" \" for doc in doc_list]  \n",
    "    doc_clean_string = ''.join(doc_clean)\n",
    "    print doc_clean_string\n",
    "    \n",
    "    #Nos cramos el contador de palabras, miramos el total de palabras y el total de palábras únicas\n",
    "    WORDS = Counter(words(doc_clean_string))\n",
    "    WORDS_FREC = Counter(words(doc_clean_string))\n",
    "    print  'Number of different words: ' ,len(WORDS.keys())\n",
    "    print  'Sum of values of words:', sum(WORDS.values())\n",
    "\n",
    "    #print WORDS \n",
    "\n",
    "    total_words = sum(WORDS.values())\n",
    "\n",
    "    #Creamos el contador de las frecuencias de las palabras\n",
    "    for key, value in WORDS.items():\n",
    "        WORDS_FREC[key] = float (value) /  float (total_words)\n",
    "\n",
    "    \n",
    "    #print WORDS_FREC\n",
    "    \n",
    "    #Nos cramos una lista con las palabras y otra con las frecuencias\n",
    "    dic_keys = list(WORDS_FREC.keys())\n",
    "    dic_values = list(WORDS_FREC.values())\n",
    "\n",
    "    #Unimos las dos listas anteriores y la ordenamos por los valores de frecuencia de mayor a menor\n",
    "    dic_keys_values  = [(dic_keys[i], dic_values[i]) for i in range(len(dic_keys))]\n",
    "    dic_indexs = np.array(range(len(dic_keys)))\n",
    "\n",
    "    dic_keys_values_sorted =  sorted(dic_keys_values, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    #Nos cramos una lista con las palabras ordenadas y otra con las frecuencias ordenadas para pintar el \n",
    "    #histograma\n",
    "    dic_keys_sorted = [i[0] for i in dic_keys_values_sorted]\n",
    "    dic_values_sorted = [i[1] for i in dic_keys_values_sorted]\n",
    "    \n",
    "    return dic_indexs, dic_keys_sorted, dic_values_sorted,WORDS, WORDS_FREC\n",
    "\n",
    "def plot_frec(dic_indexs, dic_keys_sorted, dic_values_sorted, sample, ylim, ylabel, xlabel):\n",
    "    \n",
    "\n",
    "#Pintamos el histograma con las frecuencias de las palabras\n",
    "\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.ylim((0 , ylim))\n",
    "    plt.xlabel(xlabel)\n",
    "\n",
    "    h= plt.bar(dic_indexs[0:sample] , dic_values_sorted[0:sample], width = 1.0, color = 'g')\n",
    "\n",
    "    xticks_pos = [0.65*patch.get_width() + patch.get_xy()[0] for patch in h]\n",
    "\n",
    "    plt.xticks(xticks_pos, dic_keys_sorted[0:sample],  ha='right', rotation=45, size = 'xx-small')\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(80,80))\n",
    "    plt.show()\n",
    "\n",
    "                 \n",
    "## Calculamos las frecuencias de palabras y las ordenamos\n",
    "doc_complete = df_words2['query_spel_corr'].tolist()\n",
    "\n",
    "dic_indexs, dic_keys_sorted, dic_values_sorted,WORDS, WORDS_FREC = words_frequencies(doc_complete)\n",
    "\n",
    "## Hacemos un gráfico con las freuencias de las 50 primeras palabras más frecuentes\n",
    "plot_frec(dic_indexs, dic_keys_sorted, dic_values_sorted, 50 , 0.06, 'Words frecuencies', 'First 50 most frequent words')\n",
    "\n",
    "\"\"\"\n",
    "Buscamos los paises con más queries en Ingles\n",
    "\"\"\"\n",
    "\n",
    "df_busc_country= df_words2.groupby('country').count().sort_values( by = 'query_spel_corr', ascending = False)\n",
    "print df_busc_country\n",
    "                 \n",
    "                 \n",
    "\"\"\"\n",
    "Procesamos los descriptivos para la muestra de 20.000 registros\n",
    "\"\"\"\n",
    "\n",
    "df_words = pd.read_csv(os.path.join(base_dir,'queries123sc_sample.csv'))\n",
    "df_words1 = df_words[['customer','query_spel_corr', 'language', 'country']]\n",
    "    \n",
    "\n",
    "                 \n",
    "# Hacemos representación de la nube de palabras\n",
    "\n",
    "# Cargamos datos\n",
    "\n",
    "  ##df_words = pd.read_csv(r'D:/Capstone-Wordreader/Data/queries123sc_sample.csv')\n",
    "df_words = pd.read_csv(os.path.join(base_dir,'queries123sc_sample.csv'))\n",
    "df_words003 = df_words[['customer','query_spel_corr', 'language', 'country', 'Index']]\n",
    "\n",
    "\n",
    "# Unimos los Csvs\n",
    "\n",
    "frames = [df_words003]\n",
    "df_words003 = pd.concat(frames)\n",
    "\n",
    "\n",
    "# Seleccionamos las columans que nos interesan\n",
    "\n",
    "df_words1 = df_words003[['customer','query_spel_corr', 'language','country', 'Index']]. iloc[:50]\n",
    "doc_complete = df_words1['query_spel_corr'].tolist()\n",
    "\n",
    " \n",
    "# Tokenizamos y sacamos las stopwords\n",
    "\n",
    "tokens = nltk.word_tokenize(str(doc_complete))\n",
    "stopwords = nltk.corpus.stopwords.words('english') \n",
    "tokens_nostopwords = [w for w in tokens if not w in stopwords]\n",
    "print tokens_nostopwords\n",
    "\n",
    "\n",
    "# Quitamos las puntuaciones\n",
    "\n",
    "string.punctuation\n",
    "\n",
    "tokens_nopuncnostopwords = [w.lower() for w in tokens_nostopwords if w.isalnum()]\n",
    "print tokens_nopuncnostopwords\n",
    "\n",
    "tokens = nltk.word_tokenize(str(doc_complete))\n",
    "stopwords = nltk.corpus.stopwords.words('english') \n",
    "tokens_nostopwords = [w for w in tokens if not w in stopwords]\n",
    "print tokens_nostopwords\n",
    "\n",
    "                       \n",
    "# Creamos el word cloud\n",
    "\n",
    "YOUR_TEXT = str(' '.join(word for word in tokens_nopuncnostopwords ))\n",
    "print YOUR_TEXT\n",
    "\n",
    "tags = make_tags(get_tag_counts(YOUR_TEXT), maxsize=120)\n",
    "print tags\n",
    "create_tag_image(tags, 'cloud_large.png', size=(900, 600), fontname='Lobster')\n",
    "\n",
    "import webbrowser\n",
    "webbrowser.open('cloud_large.png') # see results\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Realizamos un análisis mediante bigramas del TOP \"n\" búsquedas de acuerdo con su frecuencia de aparición\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Importamos los CSVs y los unimos\n",
    "  ##df_words = pd.read_csv(r'C:\\Users\\apascual\\Documents\\Python_Scripts\\Worldreader\\queries123sc_sample.csv')\n",
    "df_words = pd.read_csv(os.path.join(base_dir,'queries123sc_sample.csv'))\n",
    "\n",
    "df_words003 = df_words[['customer','query_spel_corr', 'language', 'country', 'Index']]\n",
    "\n",
    "#Unimos los Csvs\n",
    "frames = [df_words003]\n",
    "df_words003 = pd.concat(frames)\n",
    "\n",
    "#Seleccionamos las columnas que nos interesan\n",
    "df_words1 = df_words003[['customer','query_spel_corr', 'language','country', 'Index']]\n",
    "doc_complete = df_words1['query_spel_corr'].tolist()\n",
    "\n",
    "#Tokenizamos y sacamos las stopwords\n",
    "tokens = nltk.word_tokenize(str(doc_complete))\n",
    "stopwords = nltk.corpus.stopwords.words('english') \n",
    "tokens_nostopwords = [w for w in tokens if not w in stopwords]\n",
    "print tokens_nostopwords\n",
    "\n",
    "#Quitamos las puntuaciones\n",
    "string.punctuation\n",
    "tokens_nopuncnostopwords = [w.lower() for w in tokens_nostopwords if w.isalnum()]\n",
    "print tokens_nopuncnostopwords\n",
    "\n",
    "#encontramos los bigramas y contamos su frecuencia\n",
    "from itertools import islice, izip\n",
    "print Counter(izip(tokens_nopuncnostopwords, islice(tokens_nopuncnostopwords, 1, None)))\n",
    "bigrams_count= Counter(izip(tokens_nopuncnostopwords, islice(tokens_nopuncnostopwords, 1, None))) \n",
    "\n",
    "#ordenamos y pintamos los bigramas\n",
    "bigrams_plot = dict(bigrams_count.most_common(10))\n",
    "print(bigrams_plot)\n",
    "labels, values = zip(*bigrams_plot.items())\n",
    "indSort = np.argsort(values)[::-1] #ordenamos valores en orden descendente\n",
    "\n",
    "# redistribuimos los datos\n",
    "labels = np.array(labels)[indSort]\n",
    "values = np.array(values)[indSort]\n",
    "indexes = np.arange(len(labels))\n",
    "bar_width = 0.35\n",
    "plt.bar(indexes, values)\n",
    "\n",
    "# añadimos etiquetas\n",
    "plt.xticks(indexes + bar_width, labels)\n",
    "\n",
    "# mostramos gráficos\n",
    "plt.show()\n",
    "                 \n",
    "                 \n",
    "print 'FINISHED'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buscador de libros correspondientes a las queries sobre la muestra de 20.000 registros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamamos a la API de Google Books para recuperar información de los  libros: título, autor, categoria y descripción:\n",
    "* Seleccionamos una submuestra de 1.000 búsquedas para trabajar con la API de Google Books (límite de consulta diaria por usuario).\n",
    "* Enviamos la submuestra a la API de Google Books.\n",
    "* Procesamos la información obtenida para identificar cuál es libro que más coincide (de un total de hasta 5 candidatos) en función de la aparición de términos en su título, autor y descripción.\n",
    "* Devolvemos un total de hasta 5 libros (siendo el primero el que consideramos best match) siempre que el porcentaje de palabras de la query que aparecen en el título del libro devuelto sea superior a un 60%. Si las palabras de la búsqueda tienen un porcentaje de aparición superior al 50% en el autor devuelto consideramos que se trata de una búsqueda por autor y la etiquetamos como tal.\n",
    "* Guardamos el fichero de resultados con los 5 libros devueltos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Definimos variables de entorno\n",
    "  # base_dir = r'D:\\Capstone-Wordreader\\Data'\n",
    "base_dir = r'/Data/'\n",
    "\n",
    "\n",
    "#Cargamos fichero de muestra\n",
    "\n",
    " ##df_words = pd.read_csv('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/queries123sc_sample.csv')\n",
    "df_words = pd.read_csv(os.path.join(base_dir,'queries123sc_sample.csv'))\n",
    "\n",
    "df_words003 = df_words[['customer','query_spel_corr', 'language', 'country', 'Index']]\n",
    "frames = [df_words003]\n",
    "df_words003 = pd.concat(frames)\n",
    "\n",
    "#Seleccionamos las columnas que nos interesan\n",
    "df_words1 = df_words003[['customer','query_spel_corr', 'language','country', 'Index']]\n",
    "doc_complete = df_words1['query_spel_corr'].tolist()\n",
    "\n",
    "#Limpiamos stop-words en inglés\n",
    "stop = get_stop_words('en')\n",
    "exclude = set(string.punctuation) \n",
    "inf = inflect.engine()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i.encode(\"utf-8\") for i in doc.lower().split() if i not in stop])\n",
    "    \n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    return punc_free\n",
    "    return punc_free\n",
    "\n",
    "def num(word):\n",
    "    if word.isdigit():\n",
    "        return inf.number_to_words(word , andword='')\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "df_words1 = df_words1.iloc[2700:2800]\n",
    "doc_clean = [clean(doc) for doc in doc_complete][2700:2800]  \n",
    "\n",
    "print doc_clean\n",
    "\n",
    "\n",
    "\n",
    "# Definimos función para buscar palabras completas en un texto\n",
    "\n",
    "def findWholeWord(w):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search  \n",
    "\n",
    "\n",
    "# Definimos función para enviar un string y obtener una lista con las palabras en singular y los números convertidos a palabra\n",
    "\n",
    "def words(text): \n",
    "    text_low = text.lower()\n",
    "    text_split = text_low.split()\n",
    "    text_final = []\n",
    "    for word in text_split:\n",
    "        word_sing = inflection.singularize(word)\n",
    "        word_num = num(word_sing)\n",
    "                                        #A veces al singularizar da problemas\n",
    "        word_final = word_num.replace(\"'\", \"\")\n",
    "        text_final.append(word_final)\n",
    "    return text_final\n",
    "\n",
    "\n",
    "# Definimos función para identificar el libro más similar de todos los libros encontrados\n",
    "\n",
    "def best_match(title_list, match_title_list, authors_list, match_authors_list, description_list , match_description_list, word_ocur_list): \n",
    "    match_title_list =[0 if x==\"error\" else x for x in match_title_list]\n",
    "    match_authors_list =[0 if x==\"error\" else x for x in match_authors_list]\n",
    "    match_description_list =[0 if x==\"error\" else x for x in match_description_list]\n",
    "    word_ocur_list =  [0 if x==\"error\" else x for x in word_ocur_list]\n",
    "    \n",
    "    max_title_index = []\n",
    "    max_title = max(match_title_list)\n",
    "    \n",
    "    for j in range(len(match_title_list)):\n",
    "        if max_title == match_title_list[j]:\n",
    "            max_title_index.append(j)\n",
    "            \n",
    "    max_authors_index = []\n",
    "    max_author= max(match_authors_list)\n",
    "    \n",
    "    for j in range(len(match_authors_list)):\n",
    "        if max_author == match_authors_list[j]:\n",
    "            max_authors_index.append(j)\n",
    "            \n",
    "    description_list_2 = []\n",
    "    max_description_index = []\n",
    "    word_ocur_list_2 =  []\n",
    "    max_word_ocur_index = []\n",
    "    author = 0\n",
    "                        #Si el match de autor es menor que 0.5 consideraremos que no busca un autor\n",
    "    if max_author <= 0.5:\n",
    "        for j in range(len(max_title_index)):\n",
    "            description_list_2.append(match_description_list[max_title_index[j]])\n",
    "    else:\n",
    "        author = 1\n",
    "        for j in range(len(max_authors_index)):\n",
    "            description_list_2.append(match_description_list[max_authors_index[j]])\n",
    "    \n",
    "    max_description = max(description_list_2)\n",
    "    \n",
    "    for j in range (len(description_list_2)):\n",
    "        if description_list_2[j] == max_description:\n",
    "            if max_author <= 0.5:\n",
    "                max_description_index.append(max_title_index[j])\n",
    "            else: \n",
    "                max_description_index.append(max_authors_index[j])\n",
    "                \n",
    "    for j in range(len(max_description_index)):\n",
    "        word_ocur_list_2.append(word_ocur_list[max_description_index[j]]) \n",
    "        \n",
    "    max_word_ocur = max(word_ocur_list_2)\n",
    "        \n",
    "    for j in range (len(word_ocur_list_2)):\n",
    "        if word_ocur_list_2[j] == max_word_ocur:\n",
    "            max_word_ocur_index.append(max_description_index[j])\n",
    "    #Comprobamos que al menos uno de los elementos devueltos no tiene match 0  \n",
    "    match_title_final = 0\n",
    "    match_author_final = 0\n",
    "    for j in max_word_ocur_index:\n",
    "        if match_title_list[j] > match_title_final:\n",
    "            match_title_final = match_title_list[j]\n",
    "        if match_authors_list[j] > match_author_final:\n",
    "            match_author_final = match_authors_list[j]\n",
    "        \n",
    "    if match_title_final == 0 and match_author_final == 0:   \n",
    "        return [], author\n",
    "    else: \n",
    "        return max_word_ocur_index, author\n",
    "    \n",
    "\n",
    "\n",
    "# Definimos función de búsqueda de información relacionada con libros contra la API de Google Books\n",
    "\n",
    "def search(value, key):\n",
    "    parms = {\"q\":value, 'key':key}\n",
    "    r = requests.get(url=\"https://www.googleapis.com/books/v1/volumes\", params=parms)\n",
    "    print r\n",
    "    print r.url\n",
    "    rj = r.json()\n",
    "    english = 'en'\n",
    "        #print rj[\"totalItems\"]\n",
    "    j = 0\n",
    "\n",
    "\n",
    "#Lanzamos el proceso de búsqueda y obtención de información de la API de Google Books\n",
    "    \n",
    "    WORDS = Counter(words(value))\n",
    "    \n",
    "    print WORDS\n",
    "    \n",
    "    kind = []\n",
    "    title = []\n",
    "    match_title = []\n",
    "    authors = []\n",
    "    match_authors = []\n",
    "    categories =[]\n",
    "    language = []\n",
    "    description = []\n",
    "    match_description = []\n",
    "    word_ocurr_description = []\n",
    "    \n",
    "    try:\n",
    "        for i in rj[\"items\"]:\n",
    "            lang =   i[\"volumeInfo\"][\"language\"]\n",
    "            if lang == english :\n",
    "                if 'book' in i[\"kind\"]:\n",
    "                    try:\n",
    "                        kind.append(i[\"kind\"])\n",
    "                    except:\n",
    "                        kind.append(\"\")\n",
    "                    try:\n",
    "                        title.append(i[\"volumeInfo\"][\"title\"])\n",
    "                    except:\n",
    "                        title.append(\"\")\n",
    "                    try:\n",
    "                        authors.append(i[\"volumeInfo\"][\"authors\"])\n",
    "                    except:\n",
    "                        authors.append(\"\") \n",
    "                    try:\n",
    "                        categories.append(i[\"volumeInfo\"][\"categories\"])\n",
    "                    except:\n",
    "                        categories.append(\"\")\n",
    "                    try:\n",
    "                        language.append(i[\"volumeInfo\"][\"language\"])\n",
    "                    except:\n",
    "                        language.append(\"\")\n",
    "                    try:\n",
    "                        description.append(i[\"volumeInfo\"][\"description\"])\n",
    "                    except:\n",
    "                        description.append(\"\")\n",
    "            \n",
    "                    j = j + 1\n",
    "            if j > 10:\n",
    "                break   \n",
    "        k = 0\n",
    "       \n",
    "        for k in range(len(title)):\n",
    "            \n",
    "            #Decode utf-8 para quitar el u'word'\n",
    "            try:\n",
    "               \n",
    "                title_list = title[k].decode(encoding='UTF-8',errors='ignore').split()\n",
    "                #quitamos signos de puntuación y stop words y volvemos a codificar en utf-8\n",
    "                title_clean = [clean(word.lower()).encode(encoding='UTF-8',errors='ignore') for word in title_list]\n",
    "                #ponemos en singular las palabras\n",
    "                title_clean_sing = [inflection.singularize(word).encode(encoding='UTF-8',errors='ignore')  for word in title_clean]\n",
    "                #convertimos los números a palabras\n",
    "                title_clean_num = [num(word).encode(encoding='UTF-8',errors='ignore') for word in title_clean_sing]\n",
    "    \n",
    "                #lo juntamos todo en un string, quitamos los vacios generados en la lista\n",
    "                title_str = \" \".join(word for word in title_clean_num if word != \"\")  \n",
    "                match_pct = 0  \n",
    "                match = 0\n",
    "               \n",
    "        \n",
    "                for word in WORDS:\n",
    "                    \n",
    "                    if findWholeWord(word)(title_str):\n",
    "                        \n",
    "                        match = match + 1\n",
    "                    \n",
    "                \n",
    "                \n",
    "                match_pct = round(float(match)/float(len(WORDS)),6)\n",
    "               \n",
    "                match_title.append(match_pct)\n",
    "            except:\n",
    "                match_title.append('error')\n",
    "            \n",
    "        \n",
    "        k = 0\n",
    "    \n",
    "        for item in authors:\n",
    "            \n",
    "            match_pct = 0  \n",
    "            match = 0\n",
    "            \n",
    "            for j in range(len(item)):\n",
    "                try:\n",
    "                    author_list = item[j].decode(encoding='UTF-8',errors='ignore').split()\n",
    "                \n",
    "                    #quitamos signos de puntuación y stop words y volvemos a codificar en utf-8\n",
    "                    author_clean = [clean(word.lower()).encode(encoding='UTF-8',errors='ignore') for word in author_list]\n",
    "                    #ponemos en singular las palabras\n",
    "                    author_clean_sing = [inflection.singularize(word).encode(encoding='UTF-8',errors='ignore')  for word in author_clean]\n",
    "                    #convertimos los números a palabras\n",
    "                    author_clean_num = [num(word).encode(encoding='UTF-8',errors='ignore') for word in author_clean_sing]\n",
    "    \n",
    "                    #lo juntamos todo en un string, quitamos los vacios generados en la lista\n",
    "                    author_str = \" \".join(word for word in author_clean_num if word != \"\")  \n",
    "    \n",
    "                    \n",
    "        \n",
    "                    for word in WORDS:\n",
    "                        \n",
    "                        if findWholeWord(word)(author_str):\n",
    "                        \n",
    "                            match = match + 1\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            match_pct = round(float(match)/float(len(WORDS)),6)\n",
    "            \n",
    "            match_authors.append(match_pct)\n",
    "                  \n",
    "                \n",
    "                \n",
    "              \n",
    "            k = k + 1 \n",
    "      \n",
    "    \n",
    "    \n",
    "        for item in description:\n",
    "            \n",
    "            k = k + 1\n",
    "            if len(item) > 3 and TextBlob(item).detect_language() == 'en':\n",
    "            #Decode utf-8 para quitar el u'word'\n",
    "                try:\n",
    "                    description_list = item.decode(encoding='UTF-8',errors='ignore').split()\n",
    "                    #quitamos signos de puntuación y stop words y volvemos a codificar en utf-8\n",
    "                    description_clean = [clean(word.lower()).encode(encoding='UTF-8',errors='ignore') for word in description_list]\n",
    "                    description_clean_sing = [inflection.singularize(word).encode(encoding='UTF-8',errors='ignore')  for word in description_clean]\n",
    "                    description_clean_num = [num(word).encode(encoding='UTF-8',errors='ignore') for word in description_clean_sing]\n",
    "    #lo juntamos todo en un string, quitamos los vacios generados en la lista\n",
    "                    description_str = \" \".join(word for word in description_clean_num if word != \"\")  \n",
    "                    \n",
    "                   \n",
    "                    match_pct = 0  \n",
    "                    match = 0\n",
    "                    word_ocurr = 0\n",
    "                    \n",
    "                    \n",
    "                    for word in WORDS:\n",
    "                       \n",
    "                        if findWholeWord(word)(description_str):\n",
    "                            match = match + 1\n",
    "                            word_ocurr = word_ocurr + description_clean_num.count(word)\n",
    "                \n",
    "                    match_pct = round(float(match)/float(len(WORDS)), 6)\n",
    "                    \n",
    "                    match_description.append(match_pct)\n",
    "                    word_ocurr_description.append(word_ocurr)\n",
    "                except:\n",
    "                \n",
    "                    match_description.append('error')\n",
    "                    word_ocurr_description.append('error')\n",
    "            else:\n",
    "                match_description.append(0)\n",
    "                word_ocurr_description.append(0)\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "   \n",
    "    return title, match_title, authors, match_authors, description, match_description, word_ocurr_description, categories\n",
    "\n",
    "\n",
    "googleapikey= \"AIzaSyA2Qp-ys1jqQ1Lfp8ZUYA-Onjy4_rrh2AQ\"\n",
    "\n",
    "def search_query(query, call_yn, result, match):\n",
    "    global title_final\n",
    "    global author_final\n",
    "    global description_final\n",
    "    global category_final\n",
    "    global author_yn_final\n",
    "    global index\n",
    "    global query_num\n",
    "    \n",
    "    global title_2\n",
    "    global title_3\n",
    "    global title_4\n",
    "    global title_5\n",
    "            \n",
    "    global author_2\n",
    "    global author_3\n",
    "    global author_4\n",
    "    global author_5\n",
    "            \n",
    "    global description_2\n",
    "    global description_3\n",
    "    global description_4\n",
    "    global description_5\n",
    "            \n",
    "    global category_2\n",
    "    global category_3\n",
    "    global category_4\n",
    "    global category_5\n",
    "            \n",
    "    title_list = []\n",
    "    match_title_list = []\n",
    "    authors_list = []\n",
    "    match_authors_list = []\n",
    "    description_list = []\n",
    "    match_description_list = []\n",
    "    word_ocur_list = []\n",
    "    categories_list = []\n",
    "    \n",
    "  \n",
    "    if call_yn == 1:\n",
    "        \n",
    "        \n",
    "        title_list, match_title_list, authors_list, match_authors_list, description_list , match_description_list, word_ocur_list, categories_list = search(query, googleapikey)\n",
    "        \n",
    "        time.sleep(3)\n",
    "        if match_title_list != []:\n",
    "            index, author_yn = best_match(title_list, match_title_list, authors_list, match_authors_list, description_list , match_description_list, word_ocur_list)\n",
    "        else:\n",
    "            index = []\n",
    "            author_yn = 0\n",
    "            \n",
    "        print title_list\n",
    "        print match_title_list\n",
    "        print match_authors_list\n",
    "        print match_description_list\n",
    "        print word_ocur_list\n",
    "    \n",
    "        \n",
    "        print index\n",
    "    \n",
    "        if index == []:\n",
    "            title_final = \"0\"\n",
    "            author_final.append(\"0\")\n",
    "            description_final.append(\"0\")\n",
    "            category_final.append(\"0\")\n",
    "            author_yn_final.append(0)\n",
    "            \n",
    "            title_2.append(\"0\")\n",
    "            title_3.append(\"0\")\n",
    "            title_4.append(\"0\")\n",
    "            title_5.append(\"0\")\n",
    "            \n",
    "            author_2.append(\"0\")\n",
    "            author_3.append(\"0\")\n",
    "            author_4.append(\"0\")\n",
    "            author_5.append(\"0\")\n",
    "            \n",
    "            description_2.append(\"0\")\n",
    "            description_3.append(\"0\")\n",
    "            description_4.append(\"0\")\n",
    "            description_5.append(\"0\")\n",
    "            \n",
    "            category_2.append(\"0\")\n",
    "            category_3.append(\"0\")\n",
    "            category_4.append(\"0\")\n",
    "            category_5.append(\"0\")\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            title_final = title_list[index[0]].encode(encoding='UTF-8',errors='ignore')\n",
    "            \n",
    "            for item in authors_list[index[0]]:\n",
    "                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "            author_final.append(authors_list[index[0]])\n",
    "            description_final.append(description_list[index[0]].encode(encoding='UTF-8',errors='ignore'))\n",
    "            \n",
    "            for item in categories_list[index[0]]:\n",
    "                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "            category_final.append(categories_list[index[0]])\n",
    "            author_yn_final.append(author_yn)\n",
    "            n = 0\n",
    "            p = 0\n",
    "            \n",
    "            \n",
    "            \n",
    "            for item in match_title_list:\n",
    "                print item\n",
    "                if item >= 0.6 and item != \"error\":\n",
    "                    print index[0]\n",
    "                    if n!= index[0]:\n",
    "                        print n, p\n",
    "                        if p == 0:\n",
    "                            print title_list[n]\n",
    "                            title_2.append(title_list[n].encode(encoding='UTF-8',errors='ignore'))\n",
    "                            description_2.append(description_list[n].encode(encoding='UTF-8',errors='ignore'))\n",
    "                            for item in authors_list[n]:\n",
    "                                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "                            author_2.append(authors_list[n])\n",
    "                            for item in categories_list[n] :\n",
    "                                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "                            category_2.append(categories_list[n])\n",
    "                            \n",
    "                            \n",
    "                        if p == 1:\n",
    "                            title_3.append(title_list[n].encode(encoding='UTF-8',errors='ignore'))\n",
    "                            description_3.append(description_list[n].encode(encoding='UTF-8',errors='ignore'))\n",
    "                            for item in authors_list[n]:\n",
    "                                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "                            author_3.append(authors_list[n])\n",
    "                            for item in categories_list[n] :\n",
    "                                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "                            category_3.append(categories_list[n])\n",
    "                            \n",
    "                        if p == 2:\n",
    "                            title_4.append(title_list[n].encode(encoding='UTF-8',errors='ignore'))\n",
    "                            description_4.append(description_list[n].encode(encoding='UTF-8',errors='ignore'))\n",
    "                            for item in authors_list[n]:\n",
    "                                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "                            author_4.append(authors_list[n])\n",
    "                            for item in categories_list[n] :\n",
    "                                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "                            category_4.append(categories_list[n])\n",
    "                            \n",
    "                        if p == 3:\n",
    "                            title_5.append(title_list[n].encode(encoding='UTF-8',errors='ignore'))\n",
    "                            description_5.append(description_list[n].encode(encoding='UTF-8',errors='ignore'))\n",
    "                            for item in authors_list[n]:\n",
    "                                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "                            author_5.append(authors_list[n])\n",
    "                            for item in categories_list[n] :\n",
    "                                item = item.encode(encoding='UTF-8',errors='ignore')\n",
    "                \n",
    "                            category_5.append(categories_list[n])\n",
    "                            \n",
    "                            \n",
    "                    \n",
    "                        if p > 3:\n",
    "                            \n",
    "                            pass\n",
    "                        p = p + 1\n",
    "                n = n +1\n",
    "            if p == 0:\n",
    "                print 'P acaba 0'\n",
    "                title_2.append(\"0\")\n",
    "                title_3.append(\"0\")\n",
    "                title_4.append(\"0\")\n",
    "                title_5.append(\"0\")\n",
    "                category_2.append(\"0\")\n",
    "                category_3.append(\"0\")\n",
    "                category_4.append(\"0\")\n",
    "                category_5.append(\"0\")\n",
    "                description_2.append(\"0\")\n",
    "                description_3.append(\"0\")\n",
    "                description_4.append(\"0\")\n",
    "                description_5.append(\"0\")\n",
    "                author_2.append(\"0\")\n",
    "                author_3.append(\"0\")\n",
    "                author_4.append(\"0\")\n",
    "                author_5.append(\"0\")\n",
    "                \n",
    "                print title_2\n",
    "            if p == 1:\n",
    "                    \n",
    "                print 'P acaba 1'\n",
    "                title_3.append(\"0\")\n",
    "                title_4.append(\"0\")\n",
    "                title_5.append(\"0\")\n",
    "                    \n",
    "                category_3.append(\"0\")\n",
    "                category_4.append(\"0\")\n",
    "                category_5.append(\"0\")\n",
    "                    \n",
    "                description_3.append(\"0\")\n",
    "                description_4.append(\"0\")\n",
    "                description_5.append(\"0\")\n",
    "                \n",
    "                author_3.append(\"0\")\n",
    "                author_4.append(\"0\")\n",
    "                author_5.append(\"0\")\n",
    "            if p == 2:\n",
    "                    \n",
    "                print 'P acaba 2'   \n",
    "                title_4.append(\"0\")\n",
    "                title_5.append(\"0\")\n",
    "                   \n",
    "                category_4.append(\"0\")\n",
    "                category_5.append(\"0\")\n",
    "                    \n",
    "                description_4.append(\"0\")\n",
    "                description_5.append(\"0\")\n",
    "                author_4.append(\"0\")\n",
    "                author_5.append(\"0\")\n",
    "            if p == 3:\n",
    "                    \n",
    "                print 'P acaba 3'                        \n",
    "                title_5.append(\"0\")\n",
    "                category_5.append(\"0\")\n",
    "                    \n",
    "                description_5.append(\"0\")\n",
    "            \n",
    "                author_5.append(\"0\")\n",
    "                \n",
    "    else:  \n",
    "        pass    \n",
    "    \n",
    "    if result == 1 and match == 1:\n",
    "        \n",
    "        \n",
    "        return title_final \n",
    "        \n",
    "    if result == 1 and match == 2:\n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        print title_2\n",
    "        \n",
    "        \n",
    "        print 'Title 2:' , title_2[k]\n",
    "        return title_2[k]\n",
    "    \n",
    "    if result == 1 and match == 3:\n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        return title_3[k]\n",
    "        \n",
    "            \n",
    "    if result == 1 and match == 4:\n",
    "        \n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        \n",
    "        if k < len(title_4):\n",
    "            \n",
    "            return title_4[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    if result == 1 and match == 5:\n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        \n",
    "        if k < len(title_5):\n",
    "            \n",
    "            return title_5[k]\n",
    "        else:\n",
    "            return \"0\"    \n",
    "        \n",
    "    if result == 2 and match == 1:\n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        return author_final [k]\n",
    "    \n",
    "    \n",
    "    if result == 2 and match == 2:\n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        if k < len(author_2):\n",
    "            \n",
    "            return author_2[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    \n",
    "    \n",
    "    if result == 2 and match == 3:\n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        if k < len(author_3):\n",
    "            \n",
    "            return author_3[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    \n",
    "    if result == 2 and match == 4:\n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        if k < len(author_4):\n",
    "            \n",
    "            return author_4[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "        \n",
    "    \n",
    "    if result == 2 and match == 5:\n",
    "        query_num = query_num + 1\n",
    "        print query_num\n",
    "        \n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        print l\n",
    "        k =   l[0]- query_inicial\n",
    "        if k < len(author_5):\n",
    "            \n",
    "            return author_5[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    \n",
    "    \n",
    "    if result == 3 and match == 1:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0] - query_inicial\n",
    "        return description_final[k]\n",
    "    \n",
    "    if result == 3 and match == 2:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0] - query_inicial\n",
    "        if k < len(description_2):\n",
    "            \n",
    "            return description_2[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    \n",
    "    if result == 3 and match == 3:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0] - query_inicial\n",
    "        if k < len(description_3):\n",
    "            \n",
    "            return description_3[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    if result == 3 and match == 4:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0] - query_inicial\n",
    "        if k < len(description_4):\n",
    "            \n",
    "            return description_4[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    if result == 3 and match == 5:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0] - query_inicial\n",
    "        if k < len(description_5):\n",
    "            \n",
    "            return description_5[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "        \n",
    "        \n",
    "    if result == 4 and match == 1:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0]- query_inicial\n",
    "        return category_final[k]\n",
    "    \n",
    "    if result == 4 and match == 2:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0]- query_inicial\n",
    "        if k < len(category_2):\n",
    "            \n",
    "            return category_2[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    \n",
    "    if result == 4 and match == 3:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0]- query_inicial\n",
    "        if k < len(category_3):\n",
    "            \n",
    "            return category_3[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    \n",
    "    if result == 4 and match == 4:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0]- query_inicial\n",
    "        if k < len(category_4):\n",
    "            \n",
    "            return category_4[k]\n",
    "        else:\n",
    "            return \"0\"\n",
    "    \n",
    "    if result == 4 and match == 5:\n",
    "        query_num = query_num + 1\n",
    "      \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0]- query_inicial\n",
    "        if k < len(category_5):\n",
    "            \n",
    "            return category_5[k]\n",
    "        else:\n",
    "            return \"0\"        \n",
    "            \n",
    "    \n",
    "        \n",
    "    if result == 5 and match == 1:\n",
    "        query_num = query_num + 1\n",
    "        \n",
    "        l = df_words1[df_words1['Index'] == query_num].index.tolist()\n",
    "        k =   l[0]- query_inicial\n",
    "        return author_yn_final[k]\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "    if result != 1 and result != 2 and result != 3 and result != 4 and result != 5:\n",
    "        print 'Error not valid result'\n",
    "        return []\n",
    "        \n",
    "\n",
    "\n",
    "print df_words1.head()\n",
    "\n",
    "\n",
    "\n",
    "index = []\n",
    "title_final = []\n",
    "author_final   = []\n",
    "description_final  = []\n",
    "category_final  = []\n",
    "author_yn_final  = []\n",
    "\n",
    "\n",
    "index = []\n",
    "title_final = \"\"\n",
    "author_final   = []\n",
    "description_final  = []\n",
    "category_final  = []\n",
    "author_yn_final  = []\n",
    "\n",
    "\n",
    "\n",
    "title_2 = []\n",
    "title_3 = []\n",
    "title_4 = []\n",
    "title_5 = []\n",
    "            \n",
    "author_2 = []\n",
    "author_3 = []\n",
    "author_4 = []\n",
    "author_5 = []\n",
    "\n",
    "description_2 = []\n",
    "description_3 = []\n",
    "description_4 = []\n",
    "description_5 = []\n",
    "            \n",
    "category_2 = []\n",
    "category_3 = []\n",
    "category_4 = []\n",
    "category_5 = []\n",
    "\n",
    "global query_inicial\n",
    "\n",
    "query_inicial = 2700\n",
    "\n",
    "\n",
    "df_words1[\"Title_1\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,1,1,1))\n",
    "\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Title_2\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,1,2))\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Title_3\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,1,3))\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Title_4\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,1,4))\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Title_5\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,1,5))\n",
    "\n",
    "\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Author_1\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,2,1))\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Author_2\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,2,2))\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Author_3\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,2,3))\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Author_4\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,2,4))\n",
    "\n",
    "query_num = query_inicial  -1\n",
    "df_words1[\"Author_5\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,2,5))\n",
    "\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Description_1\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,3,1))\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Description_2\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,3,2))\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Description_3\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,3,3))\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Description_4\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,3,4))\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Description_5\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,3,5))\n",
    "\n",
    "\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Category_1\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,4,1))\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Category_2\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,4,2))\n",
    "\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Category_3\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,4,3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Category_4\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,4,4))\n",
    "\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Category_5\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,4,5))\n",
    "\n",
    "\n",
    "query_num =  query_inicial  -1\n",
    "df_words1[\"Author_search_1\"] = df_words1['query_spel_corr'].apply(lambda query: search_query(query,0,5,1))\n",
    "\n",
    "          \n",
    "\n",
    "  ##df_words1.to_csv('C:/Users/Paqui/Programas Python/Capstone project/df_queries_books_sample_09.csv')\n",
    "df_words1.to_csv(os.path.join(base_dir,'df_queries_books_sample_09.csv'))\n",
    "                       \n",
    "print 'FINISHED'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generación modelo LDA sobre el 80% de la muestra utilizando las descripciones obtenidas de la muestra con la API Google books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testeo de modelo LDA sobre el 20% de la muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
