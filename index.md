# ![Worldreader Logo](https://comms.worldreader.org/wp-content/themes/worldreader/assets/images/logo.png) 
# Query Data Project 


Created by Cary Lewis, Aina Pascual, Patricia Araguz and Enrique Rodríguez
<br>
<a href="http://www.ub.edu/datascience/postgraduate/">UB Data Science and Big Data</a> Capstone Project
<br><br>
Thank you to Worldreader for granting us access to its data and for supporting our Capstone Project.

<ol type="I">
<li><a href="#projectbackground">Project Background</a></li>
<li><a href="#projectscope">Project Scope and Methodology</a></li>
<li><a href="#projectwork">Project Work</a></li>
<li><a href="#projectresults">Findings and Results</a></li>
<li><a href="#projectconclusions">Conclusions</a></li>
<li><a href="#projectnextsteps">Possible Next Steps</a></li>
<li><a href="#projectreferences">References</a></li>
</ol>
<br><br><br><br>

<h2><a id="projectbackground">Project Background</a></h2>
<br>
Worldreader is non-profit organization working to reduce illiteracy through its reading applications and sponsorship programs. The organization has a collection of over 40,000 books in more than 40 languages with the mission "to unlock the potential of millions of people through the use of digital books in places where access to reading material is very limited."
<br><br>
The project team approached Worldreader as their large collection generates significant amounts data and particularly through the <a href="https://www.worldreader.org/what-we-do/worldreader-mobile/">Worldreader Open Library for mobile phones application</a>.
<br><br>
Worldreader was interested in the proposal of examining their data and in particular the organization wanted to focus on the queries from their mobile application. The project's aim was to examine the search query data with Worldreader providing a dataset of search queries and the corresponding fields from their application. All data provided would be anonymized and not include personal data in any form.
<br><br><br><br>
<h2><a id="projectscope">Project Scope and Methodology</a></h2>
<br>
The Project Scope was to analyse queries made by users on the feature phone application by using clustering techniques to identify similar searches. The result of the project would be to give Worldreader a better grasp on what their users are interested in reading and algorithms that the organization could use to improve upon its search queries and results in the future. 
<br><br>
The majority of the queries consisted of short text searches, few words, requiring us to supplement the data. In order to use topic modeling techniques such as Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF), we ran the queries through the Google Books API to pull book descriptions.
<br><br>
The models used LDA and NMF are unsupervised techniques for topic discovery in large document collections. Discover different topics that a set of documents represent and how much of each topic is present in a document (or corpus). 
<br><br>
Each algorithm has a different mathematical underpinning:
<ul><li>LDA is is based on a bayesian probabilistic graphical modeling</li>
<li>NMF relies on linear algebra.</li> 
</ul>
Both algorithms take as input a bag of words matrix (i.e., each document represented as a row, with each columns containing the count of words in the corpus) and produce 2 smaller matrices: 
<ul><li>a document to topic matrix (no documents * k topics)</li>
<li>a word to topic matrix (k topics * no words) that when multiplied together reproduce the bag of words matrix with the lowest error.</li>
</ul>

The output of the derived topics generated by both models involved assigning a numeric label to the topic and printing out the top words in a topic.

NMF and LDA are not able to automatically determine the number of topics and this must be specified.

<br><br>
<h2><a id="projectwork">Project Work</a></h2>
<br><br>
For an indepth review of the project work see the Capstone's <a href="https://github.com/cnlewis/CAPE_stone/blob/master/Script_proceso_busquedas_integrado.ipynb">Jupyter notebook</a>.
<br><br>
Flow Chart of Data in Capstone Project
<img src="https://raw.githubusercontent.com/cnlewis/CAPE_stone/master/images/CAPEstone_data_process_flow.png">
<br><br>
Worldreader provided our team with 6 CSV files consisting of over 3,000,000 queries and related information.
<br><br>
<table>
<tr>
<td><b>customer,</b></td>
<td><b>country,</b></td>
<td><b>url,</b></td>
<td><b>query,</b></td>
<td><b>created_at</b></td>
</tr>
<tr>
<td>157260,</td><td>"KE",</td><td>"/Search/Results?Query=New+Testament&Language=",</td><td>"New Testament",</td><td>"2016-12-27 15:48:16.893"</td>
</tr>
<tr>
<td>157261,</td><td>"PH",</td><td>"/Search/Results?Query=circles",</td><td>"circles",</td><td>"2016-11-12 18:14:11.933"</td>
</tr>
<tr>
<td>157261,</td><td>"PH",</td><td>"/Search/Results?Query=japanese",</td><td>"japanese",</td><td>"2016-11-18 17:15:54.19"</td>
</tr>
</table>

Reviewing the fields associated with the queries with Worldreader we were told the customer ID was not stable, as sessions would break and did not permit a clear idea of individual user queries.
<br><br>
It was discussed with Worldreader if it would be beneficial to focus on certain countries and it was determined to not be necessary.



<h4>Data Cleaning and Language Detection</h4>
After loading the data:
<ul>
<li>We removed the duplicate queries of each user</li>
<ul>
<li>This permitted us to see the individual unique queries from the users - for the puprose of this work duplicates could add bias in our topic modeling work.</li>
</ul>
<li>We then also removed punctuation, irregular spacing at the beginning or end of queries, empty queries or queries that only contained numerical numbers and special characters</li>
<ul>
<li>Removing these pieces ensured our scripts would run properly without failing</li>
</ul>
<li>We used textblob and langdetect to determine query languages</li>
</ul>
<h4>Language Selection and Error Correction</h4>
We decided to use English queries as it would permit us to use the models more effectively. All other queries of the other languages were removed from our data set.
<br><br>
After selecting the english queries we had validated queries and corrected misspellings within words.
<h4>Sampling and Descriptive Stats</h4>
We needed to select a random sample of 20,000 queries to run as we were not able to run the all queries due to time and API limitations. Using the sample set of 20,000 queries gave us a statistical significance of 99% and was representative of all our query data. Using the random sample of the corrected words we could use this for comparison against the total query data.
<img src="https://raw.githubusercontent.com/cnlewis/CAPE_stone/master/images/CAPEstone_sample_significance.png">
<br><br><br>Generated a word cloud with the most used terms and identified the 10 first bigramas depending on their frequency of occurrence.
<br>
<table>
<tr>
<td>
<img src="https://raw.githubusercontent.com/cnlewis/CAPE_stone/master/images/wordcloud_finalsample.png"></td>
<td><img src="https://raw.githubusercontent.com/cnlewis/CAPE_stone/master/images/bigrams_sample.png"></td>
</tr>
</table>
<h4>Classification and Topic Modeling</h4>
Supplementing the sample of 20,000 queries with the Google Books API
<ul>
<li>Retrieved from the books: title, author, category and description with the Google Books API for each query</li>
<li>We were limited to a 1,000 daily searches of Google API (daily query limit per user).</li>
<li>Processed the information obtained to identify which book matches best (from a total of up to 5 candidates) depending on the appearance of terms in their title, author, and description.</li>
<li>We returned a total of up to 5 books (the first being the best match) provided that the percentage of words in the query that appear in the title of the book returned is greater than 60%. If the search words have a percentage of appearance higher than 50% in the returned author, we consider that it is a search by author and we labeled it as such. We would later remove the author queries from our data set as we only wanted to work with queries for titles.</li>
</ul>

<h4>Defining Number of Topics</h4>
Since both LDA and NMF need "k" number of topics to run we used the perplexity measure and an iterative approach to define the most suitable number of topics for our data. 
<br>
<br>
Low perplexity indicates that the probability distribution of the model is good at predicting the sample.

<table>
<tr><td><b>K</b></td><td><b>Perplexity</b></td></tr>
<tr><td>K=5</td><td>249.75001877</td></tr>
<tr><td>K=10</td><td>254.351936899</td></tr>
<tr><td>K=15</td><td>250.76157719</td></tr>
<tr><td>K=20</td><td>251.955603856</td></tr>
<tr><td>K=25</td><td>254.939880397</td></tr>
</table>
After seeing the results we determined that 15 was the best number of topics.

<h4>We prepared the files to generate LDA and MFN models on the descriptions obtained from the sample with the Google Books API</h4>
<ul><li>We prepared work files to generate the LDA and NMF models:</li>
<ul><li>We used 80% of the sample to construct the model (train)</li>
<li>We used 20% of the sample to validate the model (test)</li>
</ul>
</ul>
<ul>
<li>We tested the methods obtained on 20% of the sample:</li>
<ul>
     <li>We predicted about 20% of the sample with the 5 most likely topics using the LDA generated in 80%</li>
     <li>We predicted about 20% of the sample with the 5 most likely topics using the NMF generated in 80%</li>
     <li>We then generated a csv file with the predictions from both of the models</li></ul></ul>

<h2><a id="projectresults">Findings and Results</a></h2>
<br><br>
<img src="https://raw.githubusercontent.com/cnlewis/CAPE_stone/master/images/CAPEstone_LDA_complete_train_best_match.png">
<br><br>
<img src="https://raw.githubusercontent.com/cnlewis/CAPE_stone/master/images/CAPEstone_NMF_complete_train_best_match.png">
<br><br>
<img src="https://raw.githubusercontent.com/cnlewis/CAPE_stone/master/images/CAPEstone_LDA_topics_classification.png">
<br><br>
<img src"https://raw.githubusercontent.com/cnlewis/CAPE_stone/master/images/CAPEstone_NMF_complete_train_categories.png">

<h2><a id="projectconclusions">Conclusions</a></h2>
<br><br>
<ul><li>LDA and NMF use information about the word co-occurrences to extract the latent topics of the data. For this reason, they promote the predictive capacity of the titles of the books in the categories.</li>
<br><li>Topics generated by LDA are close to human understanding, in terms of grouping co-occuring words together. However, these topics may not necessarily be the ones that distinguish different groups of documents-sometimes enforcing the documents to be sparse and specific in topics may help.</li>
<br><li>The results of LDA seem unstable and are different depending on the sample of 80% chosen.</li>
<br><li>When perplexity was calculated LDA seemed to be more stable for 5 or 15 topics. 15 topics seemed to reflect more precisely the diverse range of documents.</li>
<br><li>NMF can be mostly seen as a LDA of which the parameters have been fixed to enforce a sparse solution. It may not be as flexible as LDA if you want to find multiple topics in single documents (e.g., from long articles), but it usually works better with short texts of different nature.</li>
<br><li>NMF is faster than LDA for short text analysis, its computation time is lower.</li>
<br><li>NMF seems to be a more stable model both with the best match and with all the suggestions.</li>
<br><li>For the purposes of this work, we think the results of NMF complete help us to understand better user’s type of searches and recurrent topics.</li> 

</ul>

<h2><a id="projectnextsteps">Possible Next Steps</a></h2>
<br>
<h4>Classification of queries based off user information</h4>
<ul><li>Classification of queries based off user information</li>
<Ul><li>Supplement data further with user profile information.</li>
<li>Supplement data further with information related to success or failure of the search results.</li>
</Ul>
<br>
<li>Increase sample size to see if it improves the function of the models. Test different sample sets.</li>
<br>
<li>Analysis of organization's catalog.</li>
<br>
<li>Try to find other libraries with description. We could not access to any as good as Google Books.</li>
<br>
<li>Compare the supplemented queries with the information extracted from the Google Books API against the organizational book catalog:</li>
<ul><li>Identify the catalog books in high demand</li>
<li>A list of books in high demand and not part of the catalog</li>
<li>Create a recommenders system of the catalog based off the queries</li>
</ul>
<br><li>Query Classification Methodologies:</li>
<ul><li>Test LDA removing words of low frequency.</li>
<li>Use PCA to try to find core topics within the data. Feature extraction method before NMF and LDA</li>
<li>Apply TextRank with the descriptions to extract keywords and run the models.</li>
<li>Apply technical analysis for graphs of the hyperonyms of the main search terms, using the NLTK library to retrieve WordNet hyperonyms.</li>
</Ul></Ul>

<h2><a id="projectreferences">References</a></h2>

<h4>What are the pros and cons of LDA and NMF in topic modeling?</h4> 
<ul><li>https://www.quora.com/What-are-the-pros-and-cons-of-LDA-and-NMF-in-topic-modeling</li></ul> 
 
<ul><li>https://medium.com/@aneesha/topic-modeling-with-scikit-learn-e80d33668730</li></ul> 
 
<h4>Text mining Topic Modeling</h4>
<ul><li>http://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html</li></ul>
 
<h4>Exploring Topic Coherence over many models and many topics</h4>
<ul><li>http://aclweb.org/anthology/D/D12/D12-1087.pdf</li></ul>


