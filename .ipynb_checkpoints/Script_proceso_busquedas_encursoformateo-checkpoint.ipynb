{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga Limpieza y selección de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta primera fase aplicamos los procesos necesarios para limpiar el texto y seleccionar búsquedas escritas en inglés:\n",
    "\n",
    "* Carga de datos\n",
    "* Limpieza de datos\n",
    "* Detección de idioma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named textblob",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fe211385907b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named textblob"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import langdetect as lang\n",
    "\n",
    "\"\"\"\n",
    "cargamos fichero y seleccionamos campos\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv('C:/Users/Paqui/Programas Python/Capstone project/queries003.csv')\n",
    "df1 = df[['customer','country','query']]\n",
    "\n",
    "\"\"\"\n",
    "contamos el número de búsquedas por usuarios\n",
    "\"\"\"\n",
    "\n",
    "df_busc_user= df1.groupby('customer').count()\n",
    "media_busquedas= df_busc_user.mean()\n",
    "\n",
    "\"\"\"\n",
    "contamos el número de diferentes búsquedas por usuarios\n",
    "\"\"\"\n",
    "\n",
    "df_difbusc_user= df1.groupby('customer')['query'].nunique()\n",
    "media_difbusquedas= df_difbusc_user.mean()\n",
    "\n",
    "\"\"\"\n",
    "Creamos un dataframe agrupado por customer y query\n",
    "\"\"\"\n",
    "df2 = pd.DataFrame({'count' : df1.groupby( [ \"customer\", \"country\", \"query\"] ).size()}).reset_index()\n",
    "print df2.head()\n",
    "\"\"\"\n",
    "Contamos con cuantas querys nos hemos quedado\n",
    "\"\"\"\n",
    "print \"Querys no unificadas\", df1['query'].count()\n",
    "print \"Querys unificadas\",    df2['query'].count()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Eliminamos los special caracters, menos los espacios y los apóstrofes\n",
    "Con la función strip nos quitamos los espacios al inicio y al final de la línea\n",
    "\"\"\"\n",
    "\n",
    "df2['query_2'] = df2['query'].map(lambda x: re.sub(r\"[^\\w' ]\", '', x))\n",
    "df2['query_2'] = df2['query_2'].map(lambda x: re.sub(r\"_\", '', x))\n",
    "df2['query_2'] = df2['query_2'].str.strip()\n",
    "df2.head()\n",
    "print df2.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Eliminamos las querys que contienen solo números con espocios o solo espacios y contamos cuantas tenemos\n",
    "\"\"\"\n",
    "df2['query_2'] = df2['query_2'].map(lambda x: re.sub(r\"^[0-9 ]+$\", '', x))\n",
    "print df2.head()\n",
    "\n",
    "\n",
    "print \"Querys no numéricas\",    df2['query_2'].count()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Eliminamos las filas con texto vacío\n",
    "\"\"\"\n",
    "\n",
    "df3 = df2[df2['query_2'] != \"\"]\n",
    "print \"Querys no vacías\",    df3['query_2'].count()\n",
    "\n",
    "print df3.head()\n",
    "\n",
    "\"\"\"\n",
    "Si la query tiene menos de 3 carácteres entonces usamos el langdetect porque el otro \n",
    "da error\n",
    "\"\"\"\n",
    "\n",
    "def detect_lang(query):\n",
    "    if len(query) > 3:\n",
    "        return TextBlob(query).detect_language()\n",
    "    else:\n",
    "        return lang.detect(query)\n",
    "    \n",
    "\n",
    "df3['language'] = df3['query_2'].apply(detect_lang)\n",
    "df3.head()\n",
    "df3.to_csv('C:/Users/Paqui/Programas Python/Capstone project/queries003_corrected.csv')\n",
    "\n",
    "print 'FINISHED'\n",
    "\"\"\"\n",
    "for i in xrange(312112,315655):\n",
    "    text = df3.iloc[i,3]\n",
    "    print text\n",
    "    language = lang.detect(text)\n",
    "    print i, \": \", language\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización de búsquedas en inglés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selección del idioma (inglés)\n",
    "Diccionario de palabras correctas en inglés del módulo P01\n",
    "Detecta palabras incorrectas y genera fichero de errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named P02B_Spelling_corrector6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c5ff70f98dca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:/Users/Paqui/Programas Python/Capstone project'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mP02B_Spelling_corrector6\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspelling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named P02B_Spelling_corrector6"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append('C:/Users/Paqui/Programas Python/Capstone project')\n",
    "import P02B_Spelling_corrector6 as spelling\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('C:/Users/Paqui/Programas Python/Capstone project/queries001_corrected.csv')\n",
    "\n",
    "df1 = df[['customer','query_2', 'language', 'country']]\n",
    "\n",
    "df1.head()\n",
    "\n",
    "print 'Queries con todos los idiomas:',df1['query_2'].count()\n",
    "\n",
    "df_english = df1[df1.language == 'en']\n",
    "print df_english.head()\n",
    "\n",
    "print 'Queries con idioma inglés :',df_english['query_2'].count()\n",
    "\n",
    "inputfile = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/errors001_counter', 'rb')\n",
    "\n",
    "errors = pickle.load(inputfile)\n",
    "\n",
    "errors_str = \" \".join(str(x) for x in errors)\n",
    "\n",
    "print errors_str\n",
    "\n",
    "def correct_spelling(query):\n",
    "    words = query.split()\n",
    "    query_2 = \"\"\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        corrected_word = \"\"\n",
    "        single_word = \" \" + word.lower() + \" \" \n",
    "        \"We want to match the whole single  word in the words errors string\"\n",
    "        if single_word in errors_str:\n",
    "            corrected_word = spelling.correction(word.lower())\n",
    "            query_2 = query_2 + \" \" + corrected_word\n",
    "        else: \n",
    "            query_2 = query_2 + \" \" + word\n",
    "    \n",
    "    return query_2\n",
    "\n",
    "\n",
    "df_english['query_spel_corr'] = df_english['query_2'].apply(lambda query: correct_spelling(query))\n",
    "\n",
    "\"\"\"\n",
    "Creamos un dataframe agrupado por customer y query para eliminar duplicados una vez corregido\n",
    "\"\"\"\n",
    "df_english_corr = pd.DataFrame({'count' : df_english.groupby( [ \"customer\", \"country\", \"query_spel_corr\", \"language\"] ).size()}).reset_index()\n",
    "\n",
    "\"\"\"\n",
    "Contamos con cuantas querys nos hemos quedado\n",
    "\"\"\"\n",
    "print \"Querys corregidas no unificadas\", df_english['query_spel_corr'].count()\n",
    "print \"Querys coregidas  unificadas\",    df_english_corr['query_spel_corr'].count()\n",
    "\n",
    "\n",
    "df_english.to_csv('C:/Users/Paqui/Programas Python/Capstone project/queries001_spell_corrected_comparison.csv')\n",
    "df_english_corr.to_csv('C:/Users/Paqui/Programas Python/Capstone project/queries001_spell_corrected.csv')\n",
    "\n",
    "\n",
    "print 'FINISHED'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrige palabras incorrectas con el diccionario de las palabras correctas del fichero procesado en el paso anterior (módulo P02A2)\n",
    "Calcula sugerencias de corrección (eliminando letras, insertando letras, transponiendo letras, separando letras etc) en base a la palabra conocida con más frecuencia del diccionario\n",
    "Utiliza también la función suggest de la librería enchant para hacer sugerencias en el caso en que la corrección más simple anterior no funciona\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named enchant",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-300535e8382e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0menchant\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mench\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named enchant"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import enchant as ench\n",
    "import pickle\n",
    "import nltk\n",
    "from PyDictionary import PyDictionary\n",
    "import timeit\n",
    "\n",
    "dictionary=PyDictionary()\n",
    "\n",
    "\"\"\"\n",
    "Leemos el fichero corregido y con el idioma asignado y nos quedamos con los campos que nos interesan\n",
    "\"\"\"\n",
    "df_words = pd.read_csv('C:/Users/Paqui/Programas Python/Capstone project/queries003_corrected.csv')\n",
    "df_words1 = df_words[['customer','query_2', 'language']]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Seleccionamos las queries en inglés y vemos cuantas hay\n",
    "\"\"\"\n",
    "df_words2 = df_words1[df_words1['language'] == 'en']\n",
    "\n",
    "df_words2.head()\n",
    "print \"Querys corregidas en todos los idiomas \",    df_words1['query_2'].count()\n",
    "print \"Querys en inglés \",    df_words2['query_2'].count()\n",
    "\n",
    "\"\"\"\n",
    "Creamos un variable string juntando todas las queries. Ponemos en minúsculas todas las \n",
    "palabras\n",
    "\"\"\"\n",
    "size =  int(df_words2['query_2'].count() -1)\n",
    "print size\n",
    "df_words3 = df_words2.iloc[0:size, 1]\n",
    "string = pd.DataFrame(' '.join(df_words3.tolist()), columns=['query_2'], index=[0]).iloc[0,0]\n",
    "string = string.lower()\n",
    "print string\n",
    "\n",
    "\"\"\"\n",
    "Definimos el idioma que vamos a usar para la libreria Enchant y el vocabulario inglés de \n",
    "la libreria nltk\n",
    "\"\"\"\n",
    "diction = ench.Dict(\"en_UK\")\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "\n",
    "\"\"\"\n",
    "Definimos la función que nos permitirá buscar una palabra en el diccionario de Python y ver\n",
    "si la encuentra (se puede buscar en el normal o usando la versión google)\n",
    "\"\"\"\n",
    "\n",
    "def dictionary_meaning(word):\n",
    "    try:\n",
    "        meaning_1 = dictionary.meaning(word) \n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        meaning_2 = dictionary.googlemeaning(word) \n",
    "         \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if meaning_1 == None and meaning_2 == None:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Comprobamos usando varios métodos si la palabra existe o no:\n",
    "    libreria enchant \n",
    "    libreria nltk\n",
    "    Diccionario python\n",
    "En caso que no exista en ninguno no la adjuntaremos al contador final de palabras\n",
    "Si la palabra ya ha sido chequeada entonces la adjuntmmos o no directamente\n",
    "\"\"\"\n",
    "\n",
    "WORDS = Counter(words(string))\n",
    "\n",
    "print WORDS\n",
    "string_errors = \"\"\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "inputfile_001 = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/counter', 'rb')\n",
    "\n",
    "WORDS_exist001 = Counter (pickle.load(inputfile_001))\n",
    "\n",
    "inputfile_002 = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/counter_002', 'rb')\n",
    "\n",
    "WORDS_exist002 = Counter (pickle.load(inputfile_002))\n",
    "\n",
    "WORDS_exist = WORDS_exist001 + WORDS_exist002\n",
    "\"\"\"\n",
    "Primero miramos si la palabara ya está en el diccionario procesado en pasos anteriores, si no\n",
    "Comprobamos usando varios métodos si la palabra existe o no:\n",
    "    libreria enchant \n",
    "    libreria nltk\n",
    "    Diccionario python\n",
    "En caso que no exista en ninguno no la adjuntaremos al contador final de palabras\n",
    "Si la palabra ya ha sido chequeada entonces la adjuntmmos o no directamente\n",
    "\"\"\"\n",
    "for word in WORDS:\n",
    "    if word in WORDS_exist.keys():\n",
    "        pass\n",
    "    else:\n",
    "        if diction.check(word) == True:\n",
    "            pass\n",
    "        else:\n",
    "            if word in english_vocab:\n",
    "                pass          \n",
    "            else: \n",
    "                if dictionary_meaning(word) == True: \n",
    "                    pass\n",
    "                else:\n",
    "                    string_errors = string_errors + \" \" + word\n",
    "               \n",
    "errors = string_errors.split()\n",
    "print errors\n",
    "stop_time = timeit.default_timer()    \n",
    "for word in errors:\n",
    "    del WORDS[word]\n",
    "    \n",
    "stop_time = timeit.default_timer()\n",
    "\n",
    "print'Tiempo de ejecución',  stop_time - start_time\n",
    "\n",
    "\"\"\"\n",
    "Ahora creamos el contador con todas las palabras correctas seleccionadas que nos servirá\n",
    "para corregir el spelling\n",
    "\"\"\"\n",
    "print \"WORDS FINAL\", WORDS\n",
    "\n",
    "WORDS\n",
    "with open('C:/Users/Paqui/Programas Python/Capstone project/counter_003', 'wb') as outputfile:\n",
    "    pickle.dump(WORDS , outputfile)\n",
    "    \n",
    "\n",
    "\n",
    "errors\n",
    "with open('C:/Users/Paqui/Programas Python/Capstone project/errors003_counter', 'wb') as outputfile:\n",
    "    pickle.dump(errors , outputfile)\n",
    "    \n",
    "print 'FINISHED'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica el módulo P02B para cada una de las palabras incorrectas del fichero P02A2 y crea el csv output con las queries corregidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named enchant",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1aba5899e5fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0menchant\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mench\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named enchant"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import numpy as np\n",
    "import enchant as ench\n",
    "\n",
    "\n",
    "diction = ench.Dict(\"en_UK\")\n",
    "\n",
    "\"\"\"\n",
    "Abrimos los  contadores de palabras procesados anteriormente para cada uno de los ficheros\n",
    "\"\"\"\n",
    "\n",
    "inputfile_001 = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/counter', 'rb')\n",
    "\n",
    "WORDS_001 = Counter (pickle.load(inputfile_001))\n",
    "        \n",
    "inputfile_002 = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/counter_002', 'rb')\n",
    "\n",
    "WORDS_002 = Counter (pickle.load(inputfile_002))\n",
    "\n",
    "inputfile_003 = open('C:/Users/Paqui/Programas Python/Capstone project/Counter copy/counter_003', 'rb')\n",
    "\n",
    "WORDS_003 = Counter (pickle.load(inputfile_003))\n",
    "\n",
    "\"\"\"\n",
    "Creamos un contador sumando los contadores individuales de cada archivo\n",
    "\"\"\"\n",
    "\n",
    "WORDS = WORDS_001 + WORDS_002 + WORDS_003\n",
    "\n",
    "\n",
    "print  'Number of different words: ' ,len(WORDS.keys())\n",
    "print  'Sum of values of words:', sum(WORDS.values())\n",
    "\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`. No dividimos por el número de palabras de esta forma\"\n",
    "    \"nos evitamos probabilidades cercanas a 0\"\n",
    "    return WORDS[word]\n",
    " \n",
    "def correction(word): \n",
    "    \"We transform the word to lower case\"\n",
    "    word = word.lower()\n",
    "    \"Most probable spelling correction for word.\"\n",
    "    corrected_word1 = max(candidates(word), key=P)\n",
    "    \"Probability of the word.\"\n",
    "    p1 = P(corrected_word1)\n",
    "    \n",
    "    \"Most probable spelling correction for the word splitting subset.\"\n",
    "    corrected_word2 = max_split(word)\n",
    "    \n",
    "    \"If word was not corrected using the basic corrector we use enchant suggestions and we\"\n",
    "    \"select the one with more probability to appear in our text\"\n",
    "    if corrected_word1 == word:\n",
    "        suggestions = diction.suggest(word)\n",
    "        p_aux = 0\n",
    "        p_sug = 0\n",
    "        word_sug = \"\"\n",
    "        for item  in suggestions:\n",
    "            p_aux = P(item)\n",
    "            if p_aux > p_sug:\n",
    "                p_sug = p_aux\n",
    "                word_sug = item\n",
    "        \"We select depending on probability the corrected word or the two words form the splitting subset\"\n",
    "        if corrected_word2[1] > p_sug:\n",
    "            return corrected_word2[0][0] + \" \" + corrected_word2[0][1]\n",
    "        else:\n",
    "            if p_sug > 0:\n",
    "                return word_sug\n",
    "            else:\n",
    "                return word\n",
    "    \"If word corrected with the basic corrector then we return the word\"   \n",
    "    if corrected_word1 != word:\n",
    "        \"We select depending on probability the corrected word or the two words form the splitting subset\"\n",
    "        if corrected_word2[1] > p1:\n",
    "            return corrected_word2[0][0] + \" \" + corrected_word2[0][1]\n",
    "        else:\n",
    "            return corrected_word1\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "   \n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    \n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"All word splits.\"\n",
    "    \n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    \n",
    "  \n",
    "    return splits\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "\n",
    "\n",
    "def max_split(word):\n",
    "    \"All word splits.\"\n",
    "    words_splits = splits(word)\n",
    "    \n",
    "    p = np.zeros(len(words_splits))\n",
    "    \n",
    "    \"Calculating the sum of probabilities of all word splits, the two words must have\" \n",
    "    \"probability bigger than 0 (appear at least one time in the words dictionry\"\n",
    "    \"The words must have a length bigger than one to avoid single letters\"\n",
    "    i = 0\n",
    "    for word in words_splits:\n",
    "        p_0 = P(word[0])\n",
    "        p_1 = P(word[1])\n",
    "        if (p_0 > 0 and len(word[0] )> 1)  and (p_1 >0 and len(word[1])>1):\n",
    "            p[i] = p_0 + p_1\n",
    "        else: \n",
    "            p[i] = 0\n",
    "       \n",
    "        i = i + 1\n",
    "  \n",
    "    j = p.argmax()\n",
    "\n",
    "     \n",
    "    return (words_splits[j], p[j])\n",
    "\n",
    "print correction('CHIDREN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buscamos títulos en la API de Google Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama a la API de Google Books con el texto normalizado para recuperar información del libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat May 20 13:56:37 2017\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "class gbooks():\n",
    "    googleapikey=\n",
    "\n",
    "    def search(self, value):\n",
    "        parms = {\"q\":value, 'key':self.googleapikey}\n",
    "        r = requests.get(url=\"https://www.googleapis.com/books/v1/volumes\", params=parms)\n",
    "        print r.url\n",
    "        rj = r.json()\n",
    "\n",
    "        print rj[\"totalItems\"]\n",
    "        \n",
    "        for i in rj[\"items\"]:\n",
    "            try:\n",
    "                print repr(i[\"volumeInfo\"][\"title\"])\n",
    "                print repr(i[\"volumeInfo\"][\"authors\"])\n",
    "                print repr(i[\"volumeInfo\"][\"categories\"])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    bk = gbooks()\n",
    "    bk.search(\"dorian grey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
